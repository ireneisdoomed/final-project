{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 82,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset BG + Gestos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = pd.read_csv(\"../data/processed/background.csv\")\n",
    "foreground = pd.read_csv(\"../data/processed/newTrain.csv\")\n",
    "foreground_test = pd.read_csv(\"../data/processed/newTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>92.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>...</td>\n",
       "      <td>97.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>91.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1      2     3     4      5     6     7     8     9  ...   775  \\\n",
       "0  73.0  91.0  102.0  97.0  81.0  101.0  80.0  75.0  74.0  64.0  ...  92.0   \n",
       "1  79.0  85.0  101.0  94.0  89.0   91.0  77.0  77.0  70.0  67.0  ...  97.0   \n",
       "2  68.0  86.0  102.0  89.0  87.0   88.0  71.0  75.0  70.0  66.0  ...  91.0   \n",
       "3  66.0  95.0   98.0  86.0  88.0   81.0  70.0  75.0  68.0  56.0  ...  96.0   \n",
       "4  66.0  95.0   98.0  86.0  88.0   81.0  70.0  75.0  68.0  56.0  ...  96.0   \n",
       "\n",
       "    776    777    778    779    780    781    782    783  label  \n",
       "0  88.0   90.0  181.0  213.0  207.0  171.0  170.0   97.0  other  \n",
       "1  82.0   93.0  201.0  207.0  197.0  152.0  170.0  100.0  other  \n",
       "2  82.0  121.0  218.0  198.0  192.0  159.0  168.0  101.0  other  \n",
       "3  84.0  143.0  206.0  194.0  194.0  163.0  163.0   93.0  other  \n",
       "4  84.0  143.0  206.0  194.0  194.0  163.0  163.0   93.0  other  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>197</td>\n",
       "      <td>195</td>\n",
       "      <td>196</td>\n",
       "      <td>195</td>\n",
       "      <td>197</td>\n",
       "      <td>196</td>\n",
       "      <td>195</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "      <td>...</td>\n",
       "      <td>84</td>\n",
       "      <td>65</td>\n",
       "      <td>182</td>\n",
       "      <td>213</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>142</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>146</td>\n",
       "      <td>147</td>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>178</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>198</td>\n",
       "      <td>200</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>197</td>\n",
       "      <td>198</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>231</td>\n",
       "      <td>232</td>\n",
       "      <td>234</td>\n",
       "      <td>237</td>\n",
       "      <td>238</td>\n",
       "      <td>241</td>\n",
       "      <td>243</td>\n",
       "      <td>244</td>\n",
       "      <td>248</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>66</td>\n",
       "      <td>199</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>147</td>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>152</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>152</td>\n",
       "      <td>153</td>\n",
       "      <td>154</td>\n",
       "      <td>...</td>\n",
       "      <td>174</td>\n",
       "      <td>165</td>\n",
       "      <td>166</td>\n",
       "      <td>165</td>\n",
       "      <td>166</td>\n",
       "      <td>169</td>\n",
       "      <td>167</td>\n",
       "      <td>133</td>\n",
       "      <td>135</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0     A     197     195     196     195     197     196     195     196   \n",
       "1     A     142     144     144     146     147     149     150     151   \n",
       "2     A     198     200     201     200     199     198     198     197   \n",
       "3     A     231     232     234     237     238     241     243     244   \n",
       "4     A     147     149     150     152     153     153     152     153   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     196  ...        84        65       182       213       211       212   \n",
       "1     153  ...       178       179       179       180       181       182   \n",
       "2     198  ...       100        99        99        98        99        98   \n",
       "3     248  ...        90        66       199       255       255       255   \n",
       "4     154  ...       174       165       166       165       166       169   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       212       213       213       213  \n",
       "1       182       182       183       183  \n",
       "2       100       100       101       100  \n",
       "3       255       255       255       255  \n",
       "4       167       133       135       140  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(background.head())\n",
    "display(foreground.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_background = background.drop(\"label\", axis=1)\n",
    "X_foreground = foreground.drop(\"label\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_background.columns = X_foreground.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New DFs\n",
    "\n",
    "new_fg = X_foreground\n",
    "new_fg[\"label\"] = foreground[\"label\"]\n",
    "\n",
    "new_bg = X_background\n",
    "new_bg[\"label\"] = background[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>...</td>\n",
       "      <td>179.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>198.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>231.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>147.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>...</td>\n",
       "      <td>165.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0   197.0   195.0   196.0   195.0   197.0   196.0   195.0   196.0   196.0   \n",
       "1   142.0   144.0   144.0   146.0   147.0   149.0   150.0   151.0   153.0   \n",
       "2   198.0   200.0   201.0   200.0   199.0   198.0   198.0   197.0   198.0   \n",
       "3   231.0   232.0   234.0   237.0   238.0   241.0   243.0   244.0   248.0   \n",
       "4   147.0   149.0   150.0   152.0   153.0   153.0   152.0   153.0   154.0   \n",
       "\n",
       "   pixel10  ...  pixel776  pixel777  pixel778  pixel779  pixel780  pixel781  \\\n",
       "0    196.0  ...      65.0     182.0     213.0     211.0     212.0     212.0   \n",
       "1    154.0  ...     179.0     179.0     180.0     181.0     182.0     182.0   \n",
       "2    199.0  ...      99.0      99.0      98.0      99.0      98.0     100.0   \n",
       "3    249.0  ...      66.0     199.0     255.0     255.0     255.0     255.0   \n",
       "4    154.0  ...     165.0     166.0     165.0     166.0     169.0     167.0   \n",
       "\n",
       "   pixel782  pixel783  pixel784  label  \n",
       "0     213.0     213.0     213.0      A  \n",
       "1     182.0     183.0     183.0      A  \n",
       "2     100.0     101.0     100.0      A  \n",
       "3     255.0     255.0     255.0      A  \n",
       "4     133.0     135.0     140.0      A  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([new_fg, new_bg, foreground_test])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14171, 785)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77254902, 0.76470588, 0.76862745, ..., 0.83529412, 0.83529412,\n",
       "        0.83529412],\n",
       "       [0.55686275, 0.56470588, 0.56470588, ..., 0.71372549, 0.71764706,\n",
       "        0.71764706],\n",
       "       [0.77647059, 0.78431373, 0.78823529, ..., 0.39215686, 0.39607843,\n",
       "        0.39215686],\n",
       "       ...,\n",
       "       [0.67058824, 0.67058824, 0.67058824, ..., 0.25882353, 0.23529412,\n",
       "        0.23137255],\n",
       "       [0.69803922, 0.69803922, 0.69803922, ..., 0.18823529, 0.16078431,\n",
       "        0.15294118],\n",
       "       [0.68627451, 0.69019608, 0.69019608, ..., 0.1254902 , 0.09019608,\n",
       "        0.08627451]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv(\"../data/processed/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other    5509\n",
       "A        1457\n",
       "F        1451\n",
       "Y        1450\n",
       "B        1442\n",
       "T        1434\n",
       "V        1428\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas RN"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 94,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/processed/data.csv\")\n",
    "\n",
    "X = data.drop(\"label\", axis=1)\n",
    "y = data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 95,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 96,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
<<<<<<< HEAD
    "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
=======
    "from keras.layers import Dense, Dropout, Flatten\n",
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 97,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (11336, 28, 28, 1)\n",
      "11336 train samples\n",
      "2835 test samples\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 7\n",
    "batch_size = 128\n",
    "epochs = 12\n",
    "\n",
    "# DF to np array. Keras needs one-hot encoded y for multilabel classification.\n",
    "\n",
    "X_train_vector = X_train.values.reshape((X_train.shape[0], img_rows, img_cols))\n",
    "X_test_vector = X_test.values.reshape((X_test.shape[0], img_rows, img_cols))\n",
    "\n",
    "\n",
    "# Ask keras which format to use depending on used backend and arrange data as expected\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train_vector = X_train_vector.reshape(X_train_vector.shape[0], 1, img_rows, img_cols)\n",
    "    X_test_vector = X_test_vector.reshape(X_test_vector.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train_vector = X_train_vector.reshape(X_train_vector.shape[0], img_rows, img_cols, 1)\n",
    "    X_test_vector = X_test_vector.reshape(X_test_vector.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Incoming data is in uint8. Cast the input data images to be floats in range [0.0-1.0]  \n",
    "X_train_vector = X_train_vector.astype('float32') / 255\n",
    "X_test_vector = X_test_vector.astype('float32') / 255\n",
    "\n",
    "print('x_train shape:', X_train_vector.shape)\n",
    "print(X_train_vector.shape[0], 'train samples')\n",
    "print(X_test_vector.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to class matrices\n",
    "y_train_vector = y_train.values\n",
    "y_test_vector = y_test.values"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 454,
=======
   "execution_count": 17,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the neural network proposed architecture\n",
    "model = Sequential()\n",
<<<<<<< HEAD
    "model.add(Conv2D(32, kernel_size=(20, 20),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "#model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.75))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.75))\n",
=======
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 461,
=======
   "execution_count": 18,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11336 samples, validate on 2835 samples\n",
<<<<<<< HEAD
      "Epoch 1/10\n",
      "11336/11336 [==============================] - 5s 473us/step - loss: 1.4087 - accuracy: 0.4203 - val_loss: 1.3043 - val_accuracy: 0.5072\n",
      "Epoch 2/10\n",
      "11336/11336 [==============================] - 5s 443us/step - loss: 1.3315 - accuracy: 0.4773 - val_loss: 1.0572 - val_accuracy: 0.6102\n",
      "Epoch 3/10\n",
      "11336/11336 [==============================] - 5s 431us/step - loss: 1.2250 - accuracy: 0.5358 - val_loss: 0.9268 - val_accuracy: 0.7153\n",
      "Epoch 4/10\n",
      "11336/11336 [==============================] - 5s 438us/step - loss: 1.1365 - accuracy: 0.5752 - val_loss: 0.9331 - val_accuracy: 0.7044\n",
      "Epoch 5/10\n",
      "11336/11336 [==============================] - 5s 447us/step - loss: 1.0712 - accuracy: 0.5993 - val_loss: 0.7861 - val_accuracy: 0.7601\n",
      "Epoch 6/10\n",
      "11336/11336 [==============================] - 5s 446us/step - loss: 1.0330 - accuracy: 0.6205 - val_loss: 0.7339 - val_accuracy: 0.7538\n",
      "Epoch 7/10\n",
      "11336/11336 [==============================] - 5s 439us/step - loss: 1.0153 - accuracy: 0.6266 - val_loss: 0.6361 - val_accuracy: 0.8106\n",
      "Epoch 8/10\n",
      "11336/11336 [==============================] - 5s 433us/step - loss: 0.9966 - accuracy: 0.6301 - val_loss: 0.8746 - val_accuracy: 0.6995\n",
      "Epoch 9/10\n",
      "11336/11336 [==============================] - 6s 494us/step - loss: 0.9613 - accuracy: 0.6504 - val_loss: 0.6016 - val_accuracy: 0.8247\n",
      "Epoch 10/10\n",
      "11336/11336 [==============================] - 5s 453us/step - loss: 0.9263 - accuracy: 0.6561 - val_loss: 0.5817 - val_accuracy: 0.8261\n"
=======
      "Epoch 1/30\n",
      "11336/11336 [==============================] - 28s 2ms/step - loss: 0.3294 - accuracy: 0.8927 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "11336/11336 [==============================] - 28s 2ms/step - loss: 0.0111 - accuracy: 0.9974 - val_loss: 1.7407e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "11336/11336 [==============================] - 24s 2ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 5.9647e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/30\n",
      "11336/11336 [==============================] - 24s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 2.6983e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/30\n",
      "11336/11336 [==============================] - 25s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 7.2502e-06 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "11336/11336 [==============================] - 24s 2ms/step - loss: 9.4945e-04 - accuracy: 0.9997 - val_loss: 3.3817e-06 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "11336/11336 [==============================] - 23s 2ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.7690e-06 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "11336/11336 [==============================] - 23s 2ms/step - loss: 6.3219e-04 - accuracy: 0.9998 - val_loss: 9.8811e-07 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "11336/11336 [==============================] - 23s 2ms/step - loss: 6.0151e-04 - accuracy: 0.9998 - val_loss: 6.3622e-07 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "11336/11336 [==============================] - 23s 2ms/step - loss: 1.8573e-04 - accuracy: 1.0000 - val_loss: 3.5551e-07 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "11336/11336 [==============================] - 24s 2ms/step - loss: 3.3590e-04 - accuracy: 1.0000 - val_loss: 7.7776e-07 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "11336/11336 [==============================] - 28s 2ms/step - loss: 3.1709e-04 - accuracy: 0.9999 - val_loss: 3.5147e-07 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "11336/11336 [==============================] - 28s 2ms/step - loss: 2.0319e-04 - accuracy: 1.0000 - val_loss: 1.0066e-07 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "11336/11336 [==============================] - 30s 3ms/step - loss: 1.3809e-04 - accuracy: 1.0000 - val_loss: 3.7466e-08 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "11336/11336 [==============================] - 29s 3ms/step - loss: 4.1541e-04 - accuracy: 0.9999 - val_loss: 4.4277e-08 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "11336/11336 [==============================] - 31s 3ms/step - loss: 1.3760e-04 - accuracy: 1.0000 - val_loss: 4.6674e-08 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "11336/11336 [==============================] - 25s 2ms/step - loss: 6.6074e-04 - accuracy: 0.9998 - val_loss: 2.0986e-07 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "11336/11336 [==============================] - 25s 2ms/step - loss: 2.0101e-04 - accuracy: 0.9999 - val_loss: 2.9981e-08 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "11336/11336 [==============================] - 23s 2ms/step - loss: 4.1848e-04 - accuracy: 0.9999 - val_loss: 3.2672e-08 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "11336/11336 [==============================] - 26s 2ms/step - loss: 1.7892e-04 - accuracy: 0.9999 - val_loss: 8.1910e-08 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "11336/11336 [==============================] - 30s 3ms/step - loss: 5.5031e-04 - accuracy: 0.9999 - val_loss: 5.7859e-08 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "11336/11336 [==============================] - 30s 3ms/step - loss: 2.7305e-04 - accuracy: 0.9999 - val_loss: 2.1235e-08 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "11336/11336 [==============================] - 30s 3ms/step - loss: 2.6344e-04 - accuracy: 0.9999 - val_loss: 9.6713e-09 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "11336/11336 [==============================] - 30s 3ms/step - loss: 1.9269e-04 - accuracy: 0.9999 - val_loss: 8.3209e-07 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "11336/11336 [==============================] - 30s 3ms/step - loss: 1.9885e-04 - accuracy: 0.9999 - val_loss: 1.8796e-08 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "11336/11336 [==============================] - 30s 3ms/step - loss: 9.8428e-05 - accuracy: 1.0000 - val_loss: 5.0038e-09 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "11336/11336 [==============================] - 30s 3ms/step - loss: 9.9021e-05 - accuracy: 1.0000 - val_loss: 4.1208e-09 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "11336/11336 [==============================] - 30s 3ms/step - loss: 8.7564e-05 - accuracy: 1.0000 - val_loss: 8.2837e-09 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "11336/11336 [==============================] - 30s 3ms/step - loss: 8.3660e-05 - accuracy: 1.0000 - val_loss: 4.8777e-09 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "11336/11336 [==============================] - 30s 3ms/step - loss: 1.2865e-04 - accuracy: 1.0000 - val_loss: 6.8961e-09 - val_accuracy: 1.0000\n"
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
     ]
    },
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "<keras.callbacks.callbacks.History at 0x7fe47d1cdd50>"
      ]
     },
     "execution_count": 461,
=======
       "<keras.callbacks.callbacks.History at 0x7fe480b1fa10>"
      ]
     },
     "execution_count": 18,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the NN\n",
    "batch_size = 20\n",
<<<<<<< HEAD
    "epochs = 10\n",
=======
    "epochs = 30\n",
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
    "\n",
    "model.fit(X_train_vector, y_train_vector,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
<<<<<<< HEAD
    "          validation_data=(X_test_vector, y_test_vector))"
=======
    "          validation_data=(X_test, y_test))"
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 462,
=======
   "execution_count": 23,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Test loss: 0.5816879032780884\n",
      "Test accuracy: 0.8261023163795471\n"
=======
      "Test loss: 6.896052366260763e-09\n",
      "Test accuracy: 1.0\n"
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
     ]
    }
   ],
   "source": [
    "# Evaluate the model with test data\n",
<<<<<<< HEAD
    "score = model.evaluate(X_test_vector, y_test_vector, verbose=0)\n",
=======
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-acf587e557fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# GrÃ¡fico funciÃ³n pÃ©rdida + accuracy\n",
    "\n",
    "\"\"\"import matplotlib.pyplot as plt\n",
    "\n",
    "history = model.fit(x, y, validation_split=0.25, epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segundo modelo mucho mÃ¡s grande por la noche"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 441,
=======
   "execution_count": 8,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the neural network proposed architecture\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model2.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_65_input to have 4 dimensions, but got array with shape (11336, 784)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-442-76082f5e655e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           validation_data=(X_test, y_test))\n\u001b[0m",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_65_input to have 4 dimensions, but got array with shape (11336, 784)"
     ]
=======
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11336 samples, validate on 2835 samples\n",
      "Epoch 1/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1489 - accuracy: 0.6255 - val_loss: 0.2032 - val_accuracy: 0.9520\n",
      "Epoch 2/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 0.1426 - accuracy: 0.9649 - val_loss: 0.0198 - val_accuracy: 0.9982\n",
      "Epoch 3/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 0.0352 - accuracy: 0.9923 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 4/500\n",
      "11336/11336 [==============================] - 17s 2ms/step - loss: 0.0173 - accuracy: 0.9967 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 5/500\n",
      "11336/11336 [==============================] - 15s 1ms/step - loss: 0.0081 - accuracy: 0.9989 - val_loss: 4.4424e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 3.6431e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 2.2418e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 1.5387e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 5.2690e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 5.8502e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 5.4218e-05 - val_accuracy: 1.0000\n",
      "Epoch 12/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 3.7295e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 1.9172e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 6.2014e-05 - val_accuracy: 1.0000\n",
      "Epoch 15/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 2.0551e-05 - val_accuracy: 1.0000\n",
      "Epoch 16/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 1.5084e-05 - val_accuracy: 1.0000\n",
      "Epoch 17/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.0741e-05 - val_accuracy: 1.0000\n",
      "Epoch 18/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.7311e-04 - accuracy: 1.0000 - val_loss: 4.5243e-06 - val_accuracy: 1.0000\n",
      "Epoch 19/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.3493e-04 - accuracy: 0.9999 - val_loss: 2.5470e-06 - val_accuracy: 1.0000\n",
      "Epoch 20/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.8949e-04 - accuracy: 0.9997 - val_loss: 2.9873e-06 - val_accuracy: 1.0000\n",
      "Epoch 21/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.7813e-04 - accuracy: 0.9998 - val_loss: 4.5831e-06 - val_accuracy: 1.0000\n",
      "Epoch 22/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.5153e-04 - accuracy: 0.9999 - val_loss: 6.3657e-06 - val_accuracy: 1.0000\n",
      "Epoch 23/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.2053e-04 - accuracy: 0.9998 - val_loss: 3.0844e-05 - val_accuracy: 1.0000\n",
      "Epoch 24/500\n",
      "11336/11336 [==============================] - 17s 2ms/step - loss: 6.5812e-04 - accuracy: 0.9999 - val_loss: 2.0447e-06 - val_accuracy: 1.0000\n",
      "Epoch 25/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.7887e-04 - accuracy: 1.0000 - val_loss: 1.2674e-06 - val_accuracy: 1.0000\n",
      "Epoch 26/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.4574e-04 - accuracy: 1.0000 - val_loss: 6.3778e-07 - val_accuracy: 1.0000\n",
      "Epoch 27/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.9384e-04 - accuracy: 0.9997 - val_loss: 1.3714e-06 - val_accuracy: 1.0000\n",
      "Epoch 28/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.8569e-04 - accuracy: 0.9999 - val_loss: 3.1762e-06 - val_accuracy: 1.0000\n",
      "Epoch 29/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.7564e-04 - accuracy: 0.9998 - val_loss: 2.1961e-06 - val_accuracy: 1.0000\n",
      "Epoch 30/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.8899e-04 - accuracy: 1.0000 - val_loss: 1.2175e-06 - val_accuracy: 1.0000\n",
      "Epoch 31/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.4754e-04 - accuracy: 0.9999 - val_loss: 4.6402e-06 - val_accuracy: 1.0000\n",
      "Epoch 32/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.9793e-04 - accuracy: 1.0000 - val_loss: 7.4626e-07 - val_accuracy: 1.0000\n",
      "Epoch 33/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.6528e-04 - accuracy: 0.9999 - val_loss: 4.8458e-06 - val_accuracy: 1.0000\n",
      "Epoch 34/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.9528e-04 - accuracy: 0.9999 - val_loss: 9.2833e-07 - val_accuracy: 1.0000\n",
      "Epoch 35/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.8555e-04 - accuracy: 1.0000 - val_loss: 5.7681e-07 - val_accuracy: 1.0000\n",
      "Epoch 36/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.1069e-04 - accuracy: 1.0000 - val_loss: 6.1127e-07 - val_accuracy: 1.0000\n",
      "Epoch 37/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.4489e-04 - accuracy: 1.0000 - val_loss: 2.0271e-06 - val_accuracy: 1.0000\n",
      "Epoch 38/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7772e-04 - accuracy: 1.0000 - val_loss: 2.6125e-07 - val_accuracy: 1.0000\n",
      "Epoch 39/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.4837e-04 - accuracy: 0.9999 - val_loss: 6.6116e-07 - val_accuracy: 1.0000\n",
      "Epoch 40/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7490e-04 - accuracy: 1.0000 - val_loss: 3.9356e-07 - val_accuracy: 1.0000\n",
      "Epoch 41/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.4095e-04 - accuracy: 0.9999 - val_loss: 2.0978e-07 - val_accuracy: 1.0000\n",
      "Epoch 42/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.4827e-04 - accuracy: 0.9998 - val_loss: 1.6474e-07 - val_accuracy: 1.0000\n",
      "Epoch 43/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.4269e-04 - accuracy: 0.9998 - val_loss: 2.4291e-07 - val_accuracy: 1.0000\n",
      "Epoch 44/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.4371e-04 - accuracy: 1.0000 - val_loss: 1.2573e-07 - val_accuracy: 1.0000\n",
      "Epoch 45/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.6971e-04 - accuracy: 0.9999 - val_loss: 2.9240e-07 - val_accuracy: 1.0000\n",
      "Epoch 46/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.7449e-04 - accuracy: 1.0000 - val_loss: 2.0427e-07 - val_accuracy: 1.0000\n",
      "Epoch 47/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.3972e-04 - accuracy: 1.0000 - val_loss: 3.9365e-07 - val_accuracy: 1.0000\n",
      "Epoch 48/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.5589e-04 - accuracy: 0.9999 - val_loss: 2.7772e-07 - val_accuracy: 1.0000\n",
      "Epoch 49/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2199e-04 - accuracy: 1.0000 - val_loss: 1.3422e-07 - val_accuracy: 1.0000\n",
      "Epoch 50/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1216e-04 - accuracy: 1.0000 - val_loss: 1.0403e-07 - val_accuracy: 1.0000\n",
      "Epoch 51/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0001e-04 - accuracy: 1.0000 - val_loss: 5.3360e-08 - val_accuracy: 1.0000\n",
      "Epoch 52/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1011e-04 - accuracy: 1.0000 - val_loss: 1.1723e-06 - val_accuracy: 1.0000\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.3891e-05 - accuracy: 1.0000 - val_loss: 2.2234e-07 - val_accuracy: 1.0000\n",
      "Epoch 54/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0878e-04 - accuracy: 1.0000 - val_loss: 3.0838e-07 - val_accuracy: 1.0000\n",
      "Epoch 55/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0871e-04 - accuracy: 1.0000 - val_loss: 6.7527e-08 - val_accuracy: 1.0000\n",
      "Epoch 56/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2029e-04 - accuracy: 1.0000 - val_loss: 4.7598e-08 - val_accuracy: 1.0000\n",
      "Epoch 57/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1331e-04 - accuracy: 1.0000 - val_loss: 2.4516e-07 - val_accuracy: 1.0000\n",
      "Epoch 58/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7181e-04 - accuracy: 1.0000 - val_loss: 5.4495e-08 - val_accuracy: 1.0000\n",
      "Epoch 59/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.1260e-04 - accuracy: 0.9999 - val_loss: 4.3738e-07 - val_accuracy: 1.0000\n",
      "Epoch 60/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.2287e-04 - accuracy: 0.9999 - val_loss: 5.3192e-08 - val_accuracy: 1.0000\n",
      "Epoch 61/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.3991e-05 - accuracy: 1.0000 - val_loss: 1.2172e-07 - val_accuracy: 1.0000\n",
      "Epoch 62/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.4513e-05 - accuracy: 1.0000 - val_loss: 3.1494e-08 - val_accuracy: 1.0000\n",
      "Epoch 63/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.9221e-05 - accuracy: 1.0000 - val_loss: 2.0033e-07 - val_accuracy: 1.0000\n",
      "Epoch 64/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.6985e-05 - accuracy: 1.0000 - val_loss: 3.1579e-08 - val_accuracy: 1.0000\n",
      "Epoch 65/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.5468e-05 - accuracy: 1.0000 - val_loss: 5.2517e-08 - val_accuracy: 1.0000\n",
      "Epoch 66/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.5598e-04 - accuracy: 0.9999 - val_loss: 1.5738e-07 - val_accuracy: 1.0000\n",
      "Epoch 67/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.9629e-05 - accuracy: 1.0000 - val_loss: 1.1197e-07 - val_accuracy: 1.0000\n",
      "Epoch 68/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.0797e-05 - accuracy: 1.0000 - val_loss: 2.5902e-08 - val_accuracy: 1.0000\n",
      "Epoch 69/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.2080e-04 - accuracy: 0.9999 - val_loss: 1.9174e-08 - val_accuracy: 1.0000\n",
      "Epoch 70/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.9326e-05 - accuracy: 1.0000 - val_loss: 1.5493e-07 - val_accuracy: 1.0000\n",
      "Epoch 71/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.6805e-05 - accuracy: 1.0000 - val_loss: 3.2041e-08 - val_accuracy: 1.0000\n",
      "Epoch 72/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.3005e-04 - accuracy: 1.0000 - val_loss: 9.5070e-08 - val_accuracy: 1.0000\n",
      "Epoch 73/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2368e-04 - accuracy: 1.0000 - val_loss: 1.7033e-06 - val_accuracy: 1.0000\n",
      "Epoch 74/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.6527e-04 - accuracy: 0.9999 - val_loss: 6.1979e-08 - val_accuracy: 1.0000\n",
      "Epoch 75/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.0362e-05 - accuracy: 1.0000 - val_loss: 2.7458e-08 - val_accuracy: 1.0000\n",
      "Epoch 76/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.8328e-04 - accuracy: 0.9996 - val_loss: 4.1713e-08 - val_accuracy: 1.0000\n",
      "Epoch 77/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.4439e-05 - accuracy: 1.0000 - val_loss: 2.4220e-08 - val_accuracy: 1.0000\n",
      "Epoch 78/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.5579e-04 - accuracy: 1.0000 - val_loss: 1.0487e-07 - val_accuracy: 1.0000\n",
      "Epoch 79/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1741e-04 - accuracy: 1.0000 - val_loss: 4.7599e-08 - val_accuracy: 1.0000\n",
      "Epoch 80/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.3830e-04 - accuracy: 0.9999 - val_loss: 4.1881e-08 - val_accuracy: 1.0000\n",
      "Epoch 81/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.1744e-04 - accuracy: 0.9999 - val_loss: 6.0971e-08 - val_accuracy: 1.0000\n",
      "Epoch 82/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.3097e-05 - accuracy: 1.0000 - val_loss: 1.3565e-07 - val_accuracy: 1.0000\n",
      "Epoch 83/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.6295e-05 - accuracy: 1.0000 - val_loss: 3.1579e-08 - val_accuracy: 1.0000\n",
      "Epoch 84/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.8242e-05 - accuracy: 1.0000 - val_loss: 2.3295e-08 - val_accuracy: 1.0000\n",
      "Epoch 85/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.5358e-04 - accuracy: 1.0000 - val_loss: 3.6540e-08 - val_accuracy: 1.0000\n",
      "Epoch 86/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.1627e-05 - accuracy: 1.0000 - val_loss: 2.1950e-08 - val_accuracy: 1.0000\n",
      "Epoch 87/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.0976e-04 - accuracy: 0.9999 - val_loss: 2.7626e-08 - val_accuracy: 1.0000\n",
      "Epoch 88/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.5719e-05 - accuracy: 1.0000 - val_loss: 2.4514e-08 - val_accuracy: 1.0000\n",
      "Epoch 89/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.0130e-05 - accuracy: 1.0000 - val_loss: 3.2629e-08 - val_accuracy: 1.0000\n",
      "Epoch 90/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.4204e-05 - accuracy: 1.0000 - val_loss: 1.2825e-08 - val_accuracy: 1.0000\n",
      "Epoch 91/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.2969e-05 - accuracy: 1.0000 - val_loss: 1.3372e-08 - val_accuracy: 1.0000\n",
      "Epoch 92/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.7176e-05 - accuracy: 1.0000 - val_loss: 8.1155e-09 - val_accuracy: 1.0000\n",
      "Epoch 93/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.7030e-04 - accuracy: 0.9999 - val_loss: 2.7037e-08 - val_accuracy: 1.0000\n",
      "Epoch 94/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.7724e-05 - accuracy: 1.0000 - val_loss: 1.6315e-08 - val_accuracy: 1.0000\n",
      "Epoch 95/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.6620e-05 - accuracy: 1.0000 - val_loss: 2.0478e-08 - val_accuracy: 1.0000\n",
      "Epoch 96/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.8754e-05 - accuracy: 1.0000 - val_loss: 1.5474e-08 - val_accuracy: 1.0000\n",
      "Epoch 97/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.5403e-05 - accuracy: 1.0000 - val_loss: 6.4081e-08 - val_accuracy: 1.0000\n",
      "Epoch 98/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.6210e-05 - accuracy: 1.0000 - val_loss: 1.5810e-08 - val_accuracy: 1.0000\n",
      "Epoch 99/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.0589e-05 - accuracy: 1.0000 - val_loss: 1.1017e-08 - val_accuracy: 1.0000\n",
      "Epoch 100/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.9742e-04 - accuracy: 0.9999 - val_loss: 1.6946e-08 - val_accuracy: 1.0000\n",
      "Epoch 101/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7430e-05 - accuracy: 1.0000 - val_loss: 2.3379e-08 - val_accuracy: 1.0000\n",
      "Epoch 102/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.8129e-05 - accuracy: 1.0000 - val_loss: 1.0344e-08 - val_accuracy: 1.0000\n",
      "Epoch 103/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.0553e-04 - accuracy: 0.9999 - val_loss: 1.8544e-08 - val_accuracy: 1.0000\n",
      "Epoch 104/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.4799e-05 - accuracy: 1.0000 - val_loss: 3.0275e-08 - val_accuracy: 1.0000\n",
      "Epoch 105/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.3462e-05 - accuracy: 1.0000 - val_loss: 1.8922e-08 - val_accuracy: 1.0000\n",
      "Epoch 106/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.4051e-04 - accuracy: 0.9999 - val_loss: 5.5923e-08 - val_accuracy: 1.0000\n",
      "Epoch 107/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.4096e-05 - accuracy: 1.0000 - val_loss: 2.4851e-08 - val_accuracy: 1.0000\n",
      "Epoch 108/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.4432e-05 - accuracy: 1.0000 - val_loss: 1.6567e-08 - val_accuracy: 1.0000\n",
      "Epoch 109/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.7684e-05 - accuracy: 1.0000 - val_loss: 1.8459e-08 - val_accuracy: 1.0000\n",
      "Epoch 110/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.6178e-05 - accuracy: 1.0000 - val_loss: 6.2233e-09 - val_accuracy: 1.0000\n",
      "Epoch 111/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2833e-04 - accuracy: 0.9999 - val_loss: 9.2087e-09 - val_accuracy: 1.0000\n",
      "Epoch 112/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.8180e-04 - accuracy: 0.9999 - val_loss: 1.9207e-07 - val_accuracy: 1.0000\n",
      "Epoch 113/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.1642e-05 - accuracy: 1.0000 - val_loss: 2.5608e-08 - val_accuracy: 1.0000\n",
      "Epoch 114/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.5418e-04 - accuracy: 1.0000 - val_loss: 3.6667e-08 - val_accuracy: 1.0000\n",
      "Epoch 115/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.9976e-04 - accuracy: 0.9997 - val_loss: 8.6999e-08 - val_accuracy: 1.0000\n",
      "Epoch 116/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.8599e-05 - accuracy: 1.0000 - val_loss: 7.0305e-08 - val_accuracy: 1.0000\n",
      "Epoch 117/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.6531e-05 - accuracy: 1.0000 - val_loss: 3.0528e-08 - val_accuracy: 1.0000\n",
      "Epoch 118/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.0672e-05 - accuracy: 1.0000 - val_loss: 1.7324e-08 - val_accuracy: 1.0000\n",
      "Epoch 119/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.3311e-05 - accuracy: 1.0000 - val_loss: 1.1101e-08 - val_accuracy: 1.0000\n",
      "Epoch 120/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.7454e-05 - accuracy: 1.0000 - val_loss: 1.2026e-08 - val_accuracy: 1.0000\n",
      "Epoch 121/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.7542e-05 - accuracy: 1.0000 - val_loss: 8.4098e-09 - val_accuracy: 1.0000\n",
      "Epoch 122/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.7336e-05 - accuracy: 1.0000 - val_loss: 7.9473e-09 - val_accuracy: 1.0000\n",
      "Epoch 123/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.9772e-05 - accuracy: 1.0000 - val_loss: 7.4006e-09 - val_accuracy: 1.0000\n",
      "Epoch 124/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.1556e-05 - accuracy: 1.0000 - val_loss: 4.4993e-09 - val_accuracy: 1.0000\n",
      "Epoch 125/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.6910e-05 - accuracy: 1.0000 - val_loss: 2.7332e-09 - val_accuracy: 1.0000\n",
      "Epoch 126/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.5989e-05 - accuracy: 1.0000 - val_loss: 3.4060e-09 - val_accuracy: 1.0000\n",
      "Epoch 127/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.2050e-05 - accuracy: 0.9999 - val_loss: 9.6713e-09 - val_accuracy: 1.0000\n",
      "Epoch 128/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.7479e-04 - accuracy: 0.9998 - val_loss: 1.4872e-07 - val_accuracy: 1.0000\n",
      "Epoch 129/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.5425e-05 - accuracy: 1.0000 - val_loss: 1.0891e-08 - val_accuracy: 1.0000\n",
      "Epoch 130/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.8696e-05 - accuracy: 1.0000 - val_loss: 4.9197e-09 - val_accuracy: 1.0000\n",
      "Epoch 131/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.1381e-05 - accuracy: 1.0000 - val_loss: 1.7576e-08 - val_accuracy: 1.0000\n",
      "Epoch 132/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.5524e-04 - accuracy: 0.9999 - val_loss: 1.3918e-08 - val_accuracy: 1.0000\n",
      "Epoch 133/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.4876e-05 - accuracy: 1.0000 - val_loss: 4.6254e-09 - val_accuracy: 1.0000\n",
      "Epoch 134/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.6319e-05 - accuracy: 1.0000 - val_loss: 1.5684e-08 - val_accuracy: 1.0000\n",
      "Epoch 135/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.6522e-05 - accuracy: 1.0000 - val_loss: 3.1537e-09 - val_accuracy: 1.0000\n",
      "Epoch 136/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.8142e-04 - accuracy: 0.9999 - val_loss: 1.4171e-08 - val_accuracy: 1.0000\n",
      "Epoch 137/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.1123e-05 - accuracy: 1.0000 - val_loss: 4.4950e-08 - val_accuracy: 1.0000\n",
      "Epoch 138/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.5745e-05 - accuracy: 1.0000 - val_loss: 9.9236e-09 - val_accuracy: 1.0000\n",
      "Epoch 139/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.5671e-05 - accuracy: 1.0000 - val_loss: 4.7515e-09 - val_accuracy: 1.0000\n",
      "Epoch 140/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2784e-05 - accuracy: 1.0000 - val_loss: 3.2378e-09 - val_accuracy: 1.0000\n",
      "Epoch 141/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.5732e-05 - accuracy: 1.0000 - val_loss: 5.1720e-09 - val_accuracy: 1.0000\n",
      "Epoch 142/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.8111e-05 - accuracy: 1.0000 - val_loss: 3.9947e-09 - val_accuracy: 1.0000\n",
      "Epoch 143/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.5610e-05 - accuracy: 0.9999 - val_loss: 6.2653e-09 - val_accuracy: 1.0000\n",
      "Epoch 144/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7000e-05 - accuracy: 1.0000 - val_loss: 6.1812e-09 - val_accuracy: 1.0000\n",
      "Epoch 145/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.4587e-05 - accuracy: 1.0000 - val_loss: 4.0788e-09 - val_accuracy: 1.0000\n",
      "Epoch 146/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.8209e-05 - accuracy: 1.0000 - val_loss: 2.0604e-09 - val_accuracy: 1.0000\n",
      "Epoch 147/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.8483e-05 - accuracy: 1.0000 - val_loss: 6.1812e-09 - val_accuracy: 1.0000\n",
      "Epoch 148/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7376e-05 - accuracy: 1.0000 - val_loss: 3.1116e-09 - val_accuracy: 1.0000\n",
      "Epoch 149/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.1680e-05 - accuracy: 1.0000 - val_loss: 2.0184e-09 - val_accuracy: 1.0000\n",
      "Epoch 150/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.5397e-05 - accuracy: 1.0000 - val_loss: 2.4388e-09 - val_accuracy: 1.0000\n",
      "Epoch 151/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.8081e-05 - accuracy: 1.0000 - val_loss: 1.7240e-09 - val_accuracy: 1.0000\n",
      "Epoch 152/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.6207e-05 - accuracy: 1.0000 - val_loss: 4.0367e-09 - val_accuracy: 1.0000\n",
      "Epoch 153/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.1893e-05 - accuracy: 1.0000 - val_loss: 1.3456e-09 - val_accuracy: 1.0000\n",
      "Epoch 154/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1135e-05 - accuracy: 1.0000 - val_loss: 1.8922e-09 - val_accuracy: 1.0000\n",
      "Epoch 155/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.8642e-05 - accuracy: 1.0000 - val_loss: 1.0933e-09 - val_accuracy: 1.0000\n",
      "Epoch 156/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.8972e-05 - accuracy: 1.0000 - val_loss: 7.5688e-10 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.4673e-05 - accuracy: 1.0000 - val_loss: 2.3700e-07 - val_accuracy: 1.0000\n",
      "Epoch 158/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.3490e-04 - accuracy: 0.9999 - val_loss: 1.6399e-09 - val_accuracy: 1.0000\n",
      "Epoch 159/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.5954e-05 - accuracy: 1.0000 - val_loss: 1.4885e-08 - val_accuracy: 1.0000\n",
      "Epoch 160/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.3928e-04 - accuracy: 0.9998 - val_loss: 3.0696e-09 - val_accuracy: 1.0000\n",
      "Epoch 161/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.9804e-05 - accuracy: 1.0000 - val_loss: 1.8502e-09 - val_accuracy: 1.0000\n",
      "Epoch 162/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.8673e-04 - accuracy: 0.9999 - val_loss: 7.7370e-09 - val_accuracy: 1.0000\n",
      "Epoch 163/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.6273e-05 - accuracy: 1.0000 - val_loss: 4.1629e-09 - val_accuracy: 1.0000\n",
      "Epoch 164/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.5028e-05 - accuracy: 1.0000 - val_loss: 2.1445e-09 - val_accuracy: 1.0000\n",
      "Epoch 165/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.6800e-05 - accuracy: 1.0000 - val_loss: 6.1812e-09 - val_accuracy: 1.0000\n",
      "Epoch 166/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.9600e-05 - accuracy: 1.0000 - val_loss: 3.9947e-09 - val_accuracy: 1.0000\n",
      "Epoch 167/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.2151e-05 - accuracy: 1.0000 - val_loss: 2.1025e-09 - val_accuracy: 1.0000\n",
      "Epoch 168/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.3284e-05 - accuracy: 1.0000 - val_loss: 1.3035e-08 - val_accuracy: 1.0000\n",
      "Epoch 169/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0430e-04 - accuracy: 0.9999 - val_loss: 3.2378e-09 - val_accuracy: 1.0000\n",
      "Epoch 170/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2930e-05 - accuracy: 1.0000 - val_loss: 1.0933e-09 - val_accuracy: 1.0000\n",
      "Epoch 171/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.1441e-05 - accuracy: 1.0000 - val_loss: 6.0130e-09 - val_accuracy: 1.0000\n",
      "Epoch 172/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1476e-05 - accuracy: 1.0000 - val_loss: 2.4388e-09 - val_accuracy: 1.0000\n",
      "Epoch 173/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.8041e-05 - accuracy: 1.0000 - val_loss: 1.8081e-09 - val_accuracy: 1.0000\n",
      "Epoch 174/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.5490e-05 - accuracy: 1.0000 - val_loss: 1.4297e-09 - val_accuracy: 1.0000\n",
      "Epoch 175/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.7729e-06 - accuracy: 1.0000 - val_loss: 6.3074e-10 - val_accuracy: 1.0000\n",
      "Epoch 176/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.7938e-05 - accuracy: 1.0000 - val_loss: 1.0933e-09 - val_accuracy: 1.0000\n",
      "Epoch 177/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.4540e-05 - accuracy: 1.0000 - val_loss: 3.4480e-09 - val_accuracy: 1.0000\n",
      "Epoch 178/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1626e-05 - accuracy: 1.0000 - val_loss: 2.4809e-09 - val_accuracy: 1.0000\n",
      "Epoch 179/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.5927e-06 - accuracy: 1.0000 - val_loss: 1.3035e-09 - val_accuracy: 1.0000\n",
      "Epoch 180/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7782e-04 - accuracy: 0.9999 - val_loss: 3.7003e-09 - val_accuracy: 1.0000\n",
      "Epoch 181/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.7824e-05 - accuracy: 1.0000 - val_loss: 1.0933e-09 - val_accuracy: 1.0000\n",
      "Epoch 182/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.0426e-05 - accuracy: 1.0000 - val_loss: 2.5650e-09 - val_accuracy: 1.0000\n",
      "Epoch 183/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.9958e-05 - accuracy: 1.0000 - val_loss: 4.8356e-09 - val_accuracy: 1.0000\n",
      "Epoch 184/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.3309e-05 - accuracy: 1.0000 - val_loss: 1.4297e-09 - val_accuracy: 1.0000\n",
      "Epoch 185/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.6675e-05 - accuracy: 1.0000 - val_loss: 5.8869e-10 - val_accuracy: 1.0000\n",
      "Epoch 186/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.3811e-05 - accuracy: 1.0000 - val_loss: 7.5688e-10 - val_accuracy: 1.0000\n",
      "Epoch 187/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.4754e-05 - accuracy: 1.0000 - val_loss: 6.7278e-09 - val_accuracy: 1.0000\n",
      "Epoch 188/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.1409e-04 - accuracy: 0.9999 - val_loss: 2.7878e-08 - val_accuracy: 1.0000\n",
      "Epoch 189/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.1178e-05 - accuracy: 1.0000 - val_loss: 2.5229e-09 - val_accuracy: 1.0000\n",
      "Epoch 190/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0402e-05 - accuracy: 1.0000 - val_loss: 2.2706e-09 - val_accuracy: 1.0000\n",
      "Epoch 191/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.4448e-05 - accuracy: 1.0000 - val_loss: 9.3769e-09 - val_accuracy: 1.0000\n",
      "Epoch 192/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.2495e-05 - accuracy: 1.0000 - val_loss: 1.0092e-09 - val_accuracy: 1.0000\n",
      "Epoch 193/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0999e-05 - accuracy: 1.0000 - val_loss: 1.7661e-09 - val_accuracy: 1.0000\n",
      "Epoch 194/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.9220e-04 - accuracy: 1.0000 - val_loss: 2.9014e-09 - val_accuracy: 1.0000\n",
      "Epoch 195/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.3960e-04 - accuracy: 0.9999 - val_loss: 1.5979e-09 - val_accuracy: 1.0000\n",
      "Epoch 196/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.2353e-06 - accuracy: 1.0000 - val_loss: 1.5558e-09 - val_accuracy: 1.0000\n",
      "Epoch 197/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.0055e-05 - accuracy: 1.0000 - val_loss: 6.7279e-10 - val_accuracy: 1.0000\n",
      "Epoch 198/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.6238e-05 - accuracy: 1.0000 - val_loss: 2.7752e-09 - val_accuracy: 1.0000\n",
      "Epoch 199/500\n",
      "11336/11336 [==============================] - 17s 1ms/step - loss: 5.0338e-06 - accuracy: 1.0000 - val_loss: 2.1025e-09 - val_accuracy: 1.0000\n",
      "Epoch 200/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.0680e-05 - accuracy: 1.0000 - val_loss: 7.9893e-10 - val_accuracy: 1.0000\n",
      "Epoch 201/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1169e-05 - accuracy: 1.0000 - val_loss: 1.1774e-09 - val_accuracy: 1.0000\n",
      "Epoch 202/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2626e-05 - accuracy: 1.0000 - val_loss: 7.6108e-09 - val_accuracy: 1.0000\n",
      "Epoch 203/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.4801e-04 - accuracy: 0.9999 - val_loss: 1.2615e-09 - val_accuracy: 1.0000\n",
      "Epoch 204/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.2303e-05 - accuracy: 1.0000 - val_loss: 7.9893e-10 - val_accuracy: 1.0000\n",
      "Epoch 205/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.6296e-05 - accuracy: 1.0000 - val_loss: 1.3035e-09 - val_accuracy: 1.0000\n",
      "Epoch 206/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.0345e-05 - accuracy: 1.0000 - val_loss: 1.4297e-09 - val_accuracy: 1.0000\n",
      "Epoch 207/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.6282e-05 - accuracy: 1.0000 - val_loss: 4.6254e-09 - val_accuracy: 1.0000\n",
      "Epoch 208/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7343e-05 - accuracy: 1.0000 - val_loss: 7.9893e-10 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.8197e-05 - accuracy: 1.0000 - val_loss: 1.8502e-09 - val_accuracy: 1.0000\n",
      "Epoch 210/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.5421e-05 - accuracy: 1.0000 - val_loss: 2.8173e-09 - val_accuracy: 1.0000\n",
      "Epoch 211/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.0622e-05 - accuracy: 1.0000 - val_loss: 2.2286e-09 - val_accuracy: 1.0000\n",
      "Epoch 212/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.5410e-04 - accuracy: 0.9999 - val_loss: 1.1816e-08 - val_accuracy: 1.0000\n",
      "Epoch 213/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.8826e-05 - accuracy: 1.0000 - val_loss: 2.8173e-09 - val_accuracy: 1.0000\n",
      "Epoch 214/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.7698e-05 - accuracy: 1.0000 - val_loss: 5.4664e-10 - val_accuracy: 1.0000\n",
      "Epoch 215/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2464e-05 - accuracy: 1.0000 - val_loss: 6.3074e-10 - val_accuracy: 1.0000\n",
      "Epoch 216/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.9587e-04 - accuracy: 0.9999 - val_loss: 3.1537e-09 - val_accuracy: 1.0000\n",
      "Epoch 217/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.7109e-05 - accuracy: 1.0000 - val_loss: 2.9434e-10 - val_accuracy: 1.0000\n",
      "Epoch 218/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.8234e-05 - accuracy: 1.0000 - val_loss: 2.3968e-09 - val_accuracy: 1.0000\n",
      "Epoch 219/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7612e-05 - accuracy: 1.0000 - val_loss: 1.0933e-09 - val_accuracy: 1.0000\n",
      "Epoch 220/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.6738e-06 - accuracy: 1.0000 - val_loss: 8.4098e-10 - val_accuracy: 1.0000\n",
      "Epoch 221/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.6551e-05 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 222/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.3182e-05 - accuracy: 1.0000 - val_loss: 4.6254e-10 - val_accuracy: 1.0000\n",
      "Epoch 223/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.4538e-05 - accuracy: 1.0000 - val_loss: 5.0459e-10 - val_accuracy: 1.0000\n",
      "Epoch 224/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1779e-04 - accuracy: 1.0000 - val_loss: 3.5908e-08 - val_accuracy: 1.0000\n",
      "Epoch 225/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0262e-04 - accuracy: 0.9999 - val_loss: 2.3631e-08 - val_accuracy: 1.0000\n",
      "Epoch 226/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2487e-05 - accuracy: 1.0000 - val_loss: 2.1445e-09 - val_accuracy: 1.0000\n",
      "Epoch 227/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.6764e-05 - accuracy: 1.0000 - val_loss: 3.3639e-10 - val_accuracy: 1.0000\n",
      "Epoch 228/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.2157e-06 - accuracy: 1.0000 - val_loss: 4.6254e-10 - val_accuracy: 1.0000\n",
      "Epoch 229/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.9253e-05 - accuracy: 1.0000 - val_loss: 1.0933e-09 - val_accuracy: 1.0000\n",
      "Epoch 230/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.8726e-05 - accuracy: 1.0000 - val_loss: 1.6861e-08 - val_accuracy: 1.0000\n",
      "Epoch 231/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.4161e-04 - accuracy: 0.9999 - val_loss: 2.9014e-09 - val_accuracy: 1.0000\n",
      "Epoch 232/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.0417e-05 - accuracy: 1.0000 - val_loss: 4.6254e-10 - val_accuracy: 1.0000\n",
      "Epoch 233/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.9215e-05 - accuracy: 1.0000 - val_loss: 2.1025e-10 - val_accuracy: 1.0000\n",
      "Epoch 234/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.3029e-05 - accuracy: 1.0000 - val_loss: 2.1865e-09 - val_accuracy: 1.0000\n",
      "Epoch 235/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.2942e-05 - accuracy: 1.0000 - val_loss: 5.4664e-10 - val_accuracy: 1.0000\n",
      "Epoch 236/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.9414e-05 - accuracy: 1.0000 - val_loss: 1.0722e-08 - val_accuracy: 1.0000\n",
      "Epoch 237/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.7440e-05 - accuracy: 1.0000 - val_loss: 2.1025e-10 - val_accuracy: 1.0000\n",
      "Epoch 238/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.7493e-05 - accuracy: 1.0000 - val_loss: 4.6254e-10 - val_accuracy: 1.0000\n",
      "Epoch 239/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.9712e-05 - accuracy: 1.0000 - val_loss: 8.8303e-10 - val_accuracy: 1.0000\n",
      "Epoch 240/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0257e-05 - accuracy: 1.0000 - val_loss: 5.0459e-10 - val_accuracy: 1.0000\n",
      "Epoch 241/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.1789e-05 - accuracy: 1.0000 - val_loss: 2.5229e-10 - val_accuracy: 1.0000\n",
      "Epoch 242/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.9007e-05 - accuracy: 1.0000 - val_loss: 2.9434e-10 - val_accuracy: 1.0000\n",
      "Epoch 243/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.3988e-06 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 244/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.7157e-06 - accuracy: 1.0000 - val_loss: 1.6820e-10 - val_accuracy: 1.0000\n",
      "Epoch 245/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.8321e-04 - accuracy: 0.9999 - val_loss: 5.6346e-09 - val_accuracy: 1.0000\n",
      "Epoch 246/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1279e-05 - accuracy: 1.0000 - val_loss: 1.1353e-09 - val_accuracy: 1.0000\n",
      "Epoch 247/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.5079e-06 - accuracy: 1.0000 - val_loss: 5.0459e-10 - val_accuracy: 1.0000\n",
      "Epoch 248/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.9681e-05 - accuracy: 1.0000 - val_loss: 5.8869e-10 - val_accuracy: 1.0000\n",
      "Epoch 249/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.3079e-05 - accuracy: 1.0000 - val_loss: 1.6820e-09 - val_accuracy: 1.0000\n",
      "Epoch 250/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.6960e-06 - accuracy: 1.0000 - val_loss: 5.8869e-10 - val_accuracy: 1.0000\n",
      "Epoch 251/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.7056e-05 - accuracy: 1.0000 - val_loss: 2.5229e-10 - val_accuracy: 1.0000\n",
      "Epoch 252/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.5608e-05 - accuracy: 0.9999 - val_loss: 1.1774e-09 - val_accuracy: 1.0000\n",
      "Epoch 253/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.8826e-05 - accuracy: 1.0000 - val_loss: 5.8869e-10 - val_accuracy: 1.0000\n",
      "Epoch 254/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.9541e-04 - accuracy: 0.9999 - val_loss: 7.5688e-10 - val_accuracy: 1.0000\n",
      "Epoch 255/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.1195e-05 - accuracy: 1.0000 - val_loss: 5.4664e-10 - val_accuracy: 1.0000\n",
      "Epoch 256/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.8442e-05 - accuracy: 1.0000 - val_loss: 9.2508e-10 - val_accuracy: 1.0000\n",
      "Epoch 257/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.5092e-05 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 258/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.2816e-05 - accuracy: 1.0000 - val_loss: 4.2049e-10 - val_accuracy: 1.0000\n",
      "Epoch 259/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.5892e-06 - accuracy: 1.0000 - val_loss: 3.3639e-10 - val_accuracy: 1.0000\n",
      "Epoch 260/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.6082e-05 - accuracy: 1.0000 - val_loss: 1.6820e-10 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 261/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.4135e-05 - accuracy: 1.0000 - val_loss: 2.5229e-10 - val_accuracy: 1.0000\n",
      "Epoch 262/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.4691e-05 - accuracy: 1.0000 - val_loss: 2.5229e-10 - val_accuracy: 1.0000\n",
      "Epoch 263/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.0072e-05 - accuracy: 1.0000 - val_loss: 3.7844e-10 - val_accuracy: 1.0000\n",
      "Epoch 264/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2799e-05 - accuracy: 1.0000 - val_loss: 1.6820e-10 - val_accuracy: 1.0000\n",
      "Epoch 265/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.0428e-06 - accuracy: 1.0000 - val_loss: 2.1025e-10 - val_accuracy: 1.0000\n",
      "Epoch 266/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.4463e-05 - accuracy: 1.0000 - val_loss: 4.6254e-10 - val_accuracy: 1.0000\n",
      "Epoch 267/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.8389e-05 - accuracy: 1.0000 - val_loss: 5.0459e-10 - val_accuracy: 1.0000\n",
      "Epoch 268/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.4419e-05 - accuracy: 1.0000 - val_loss: 9.2508e-10 - val_accuracy: 1.0000\n",
      "Epoch 269/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.0102e-06 - accuracy: 1.0000 - val_loss: 3.3639e-10 - val_accuracy: 1.0000\n",
      "Epoch 270/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.5570e-05 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 271/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.1429e-05 - accuracy: 1.0000 - val_loss: 3.7844e-10 - val_accuracy: 1.0000\n",
      "Epoch 272/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.6229e-05 - accuracy: 1.0000 - val_loss: 2.5229e-10 - val_accuracy: 1.0000\n",
      "Epoch 273/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.2509e-06 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 274/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.2670e-06 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 275/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.1678e-05 - accuracy: 0.9999 - val_loss: 1.6820e-10 - val_accuracy: 1.0000\n",
      "Epoch 276/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1213e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 277/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.8925e-06 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 278/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.0707e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 279/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2043e-04 - accuracy: 0.9999 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 280/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1909e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 281/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.2677e-05 - accuracy: 1.0000 - val_loss: 2.1025e-10 - val_accuracy: 1.0000\n",
      "Epoch 282/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.4760e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 283/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.9950e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 284/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0507e-05 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 285/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.5009e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 286/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.4412e-05 - accuracy: 1.0000 - val_loss: 3.7844e-10 - val_accuracy: 1.0000\n",
      "Epoch 287/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.8763e-05 - accuracy: 1.0000 - val_loss: 3.3639e-10 - val_accuracy: 1.0000\n",
      "Epoch 288/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7721e-05 - accuracy: 1.0000 - val_loss: 1.2138e-07 - val_accuracy: 1.0000\n",
      "Epoch 289/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.9618e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 290/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.6744e-04 - accuracy: 0.9999 - val_loss: 1.4212e-08 - val_accuracy: 1.0000\n",
      "Epoch 291/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.6634e-05 - accuracy: 0.9999 - val_loss: 2.3547e-09 - val_accuracy: 1.0000\n",
      "Epoch 292/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.8202e-05 - accuracy: 1.0000 - val_loss: 2.2286e-09 - val_accuracy: 1.0000\n",
      "Epoch 293/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2598e-04 - accuracy: 0.9999 - val_loss: 1.4002e-08 - val_accuracy: 1.0000\n",
      "Epoch 294/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.3605e-05 - accuracy: 1.0000 - val_loss: 2.1361e-08 - val_accuracy: 1.0000\n",
      "Epoch 295/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.6806e-05 - accuracy: 1.0000 - val_loss: 1.4717e-09 - val_accuracy: 1.0000\n",
      "Epoch 296/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.2221e-05 - accuracy: 1.0000 - val_loss: 1.0092e-09 - val_accuracy: 1.0000\n",
      "Epoch 297/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.6021e-05 - accuracy: 1.0000 - val_loss: 8.6200e-09 - val_accuracy: 1.0000\n",
      "Epoch 298/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.2438e-05 - accuracy: 1.0000 - val_loss: 1.8502e-09 - val_accuracy: 1.0000\n",
      "Epoch 299/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.4623e-06 - accuracy: 1.0000 - val_loss: 2.5229e-10 - val_accuracy: 1.0000\n",
      "Epoch 300/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.5919e-05 - accuracy: 1.0000 - val_loss: 2.1025e-10 - val_accuracy: 1.0000\n",
      "Epoch 301/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.3484e-06 - accuracy: 1.0000 - val_loss: 2.1025e-10 - val_accuracy: 1.0000\n",
      "Epoch 302/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.5621e-05 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 303/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7923e-05 - accuracy: 1.0000 - val_loss: 2.1025e-10 - val_accuracy: 1.0000\n",
      "Epoch 304/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.6645e-05 - accuracy: 1.0000 - val_loss: 3.9106e-09 - val_accuracy: 1.0000\n",
      "Epoch 305/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.5966e-06 - accuracy: 1.0000 - val_loss: 7.5688e-10 - val_accuracy: 1.0000\n",
      "Epoch 306/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1012e-05 - accuracy: 1.0000 - val_loss: 4.2049e-10 - val_accuracy: 1.0000\n",
      "Epoch 307/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.4574e-04 - accuracy: 0.9999 - val_loss: 5.4664e-10 - val_accuracy: 1.0000\n",
      "Epoch 308/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.7196e-06 - accuracy: 1.0000 - val_loss: 3.7844e-10 - val_accuracy: 1.0000\n",
      "Epoch 309/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.5926e-06 - accuracy: 1.0000 - val_loss: 2.1025e-10 - val_accuracy: 1.0000\n",
      "Epoch 310/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7573e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 311/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.5276e-05 - accuracy: 0.9999 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 312/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1188e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.9136e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 314/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.0929e-05 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 315/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.4984e-06 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 316/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.4438e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 317/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.2460e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 318/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.8388e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 319/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.3154e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 320/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1306e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 321/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.5220e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 322/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.5723e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 323/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.6648e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 324/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.0553e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 325/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.1575e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 326/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.8332e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 327/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.5124e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 328/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.8712e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 329/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.6665e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 330/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.5230e-04 - accuracy: 0.9999 - val_loss: 2.0604e-09 - val_accuracy: 1.0000\n",
      "Epoch 331/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.8982e-06 - accuracy: 1.0000 - val_loss: 6.7279e-10 - val_accuracy: 1.0000\n",
      "Epoch 332/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.8184e-06 - accuracy: 1.0000 - val_loss: 1.6820e-10 - val_accuracy: 1.0000\n",
      "Epoch 333/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.5113e-06 - accuracy: 1.0000 - val_loss: 1.0092e-09 - val_accuracy: 1.0000\n",
      "Epoch 334/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.6164e-05 - accuracy: 1.0000 - val_loss: 1.6820e-10 - val_accuracy: 1.0000\n",
      "Epoch 335/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.9697e-06 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 336/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7291e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 337/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.3330e-04 - accuracy: 0.9999 - val_loss: 3.7844e-10 - val_accuracy: 1.0000\n",
      "Epoch 338/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.3361e-05 - accuracy: 1.0000 - val_loss: 3.3639e-10 - val_accuracy: 1.0000\n",
      "Epoch 339/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.2793e-05 - accuracy: 1.0000 - val_loss: 4.2049e-10 - val_accuracy: 1.0000\n",
      "Epoch 340/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.4107e-05 - accuracy: 1.0000 - val_loss: 1.6820e-10 - val_accuracy: 1.0000\n",
      "Epoch 341/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.8509e-04 - accuracy: 0.9999 - val_loss: 5.0459e-10 - val_accuracy: 1.0000\n",
      "Epoch 342/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0409e-05 - accuracy: 1.0000 - val_loss: 2.5229e-10 - val_accuracy: 1.0000\n",
      "Epoch 343/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.6947e-05 - accuracy: 1.0000 - val_loss: 4.6254e-10 - val_accuracy: 1.0000\n",
      "Epoch 344/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.3659e-05 - accuracy: 1.0000 - val_loss: 3.3639e-10 - val_accuracy: 1.0000\n",
      "Epoch 345/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.4872e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 346/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.1839e-05 - accuracy: 1.0000 - val_loss: 3.9526e-09 - val_accuracy: 1.0000\n",
      "Epoch 347/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.3187e-05 - accuracy: 1.0000 - val_loss: 3.6162e-09 - val_accuracy: 1.0000\n",
      "Epoch 348/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.8410e-05 - accuracy: 1.0000 - val_loss: 8.9984e-09 - val_accuracy: 1.0000\n",
      "Epoch 349/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.3060e-06 - accuracy: 1.0000 - val_loss: 7.9893e-09 - val_accuracy: 1.0000\n",
      "Epoch 350/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.7656e-06 - accuracy: 1.0000 - val_loss: 1.4717e-09 - val_accuracy: 1.0000\n",
      "Epoch 351/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.3277e-05 - accuracy: 1.0000 - val_loss: 7.1483e-10 - val_accuracy: 1.0000\n",
      "Epoch 352/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1326e-05 - accuracy: 1.0000 - val_loss: 5.4664e-10 - val_accuracy: 1.0000\n",
      "Epoch 353/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.1191e-05 - accuracy: 1.0000 - val_loss: 2.1025e-10 - val_accuracy: 1.0000\n",
      "Epoch 354/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1955e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 355/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.7058e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 356/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.6992e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 357/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7775e-05 - accuracy: 1.0000 - val_loss: 2.5229e-10 - val_accuracy: 1.0000\n",
      "Epoch 358/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1293e-05 - accuracy: 1.0000 - val_loss: 1.1774e-09 - val_accuracy: 1.0000\n",
      "Epoch 359/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.4483e-06 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 360/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.9336e-06 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 361/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.3256e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 362/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.5534e-06 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 363/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.8286e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 364/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.4648e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 365/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.2418e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 366/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.5452e-05 - accuracy: 1.0000 - val_loss: 2.9434e-10 - val_accuracy: 1.0000\n",
      "Epoch 367/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.0880e-05 - accuracy: 0.9999 - val_loss: 8.4098e-10 - val_accuracy: 1.0000\n",
      "Epoch 368/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2492e-05 - accuracy: 1.0000 - val_loss: 4.2049e-10 - val_accuracy: 1.0000\n",
      "Epoch 369/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.3534e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 370/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.9881e-06 - accuracy: 1.0000 - val_loss: 1.6820e-10 - val_accuracy: 1.0000\n",
      "Epoch 371/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.9235e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 372/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.2434e-05 - accuracy: 0.9999 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 373/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.2003e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 374/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0646e-05 - accuracy: 1.0000 - val_loss: 3.7844e-10 - val_accuracy: 1.0000\n",
      "Epoch 375/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.2653e-06 - accuracy: 1.0000 - val_loss: 3.7844e-10 - val_accuracy: 1.0000\n",
      "Epoch 376/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.9741e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 377/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7877e-05 - accuracy: 1.0000 - val_loss: 2.9434e-10 - val_accuracy: 1.0000\n",
      "Epoch 378/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.9352e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 379/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0506e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 380/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1644e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 381/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.8052e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 382/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.4262e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 383/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.8352e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 384/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2575e-05 - accuracy: 1.0000 - val_loss: 1.6820e-10 - val_accuracy: 1.0000\n",
      "Epoch 385/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.5842e-06 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 386/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.9892e-06 - accuracy: 1.0000 - val_loss: 1.4297e-09 - val_accuracy: 1.0000\n",
      "Epoch 387/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.8593e-06 - accuracy: 1.0000 - val_loss: 1.2194e-09 - val_accuracy: 1.0000\n",
      "Epoch 388/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.0989e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 389/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.6490e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 390/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.0020e-05 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 391/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.8122e-06 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 392/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1926e-06 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 393/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1378e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 394/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.6076e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 395/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.5997e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 396/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.3222e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 397/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.7289e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 398/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.3057e-05 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 399/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.6048e-06 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 400/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.4814e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 401/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.8048e-05 - accuracy: 1.0000 - val_loss: 1.0512e-09 - val_accuracy: 1.0000\n",
      "Epoch 402/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.7515e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 403/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.9591e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 404/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.9724e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 405/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.2656e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 406/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.7692e-05 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 407/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.6244e-06 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 408/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.9133e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 409/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.3715e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 410/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.4839e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 411/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.6304e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 412/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.6708e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 413/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.0824e-04 - accuracy: 0.9999 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 414/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.3296e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 415/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.3922e-05 - accuracy: 1.0000 - val_loss: 4.6254e-10 - val_accuracy: 1.0000\n",
      "Epoch 416/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.8308e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 417/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.3045e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 418/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2865e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 419/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2454e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 420/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.2415e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 421/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.4541e-04 - accuracy: 0.9999 - val_loss: 7.7790e-09 - val_accuracy: 1.0000\n",
      "Epoch 422/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.1934e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 423/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.3327e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 424/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.7136e-06 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 425/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.5636e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 426/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.8706e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 427/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1742e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 428/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.4400e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 429/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0509e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 430/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.8046e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 431/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.8407e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 432/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.7392e-05 - accuracy: 1.0000 - val_loss: 3.7424e-09 - val_accuracy: 1.0000\n",
      "Epoch 433/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.2547e-06 - accuracy: 1.0000 - val_loss: 5.4243e-09 - val_accuracy: 1.0000\n",
      "Epoch 434/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1841e-05 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 435/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.2938e-06 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 436/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.5004e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 437/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.6226e-06 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 438/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.9388e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 439/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.0134e-04 - accuracy: 0.9999 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 440/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.4205e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 441/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.3063e-05 - accuracy: 0.9999 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 442/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.3101e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 443/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.6721e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 444/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.4660e-06 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 445/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.8045e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 446/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.0256e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 447/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7993e-04 - accuracy: 0.9999 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 448/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.5947e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 449/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.6115e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 450/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.1887e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 451/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.6321e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 452/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1924e-05 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 453/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.6562e-04 - accuracy: 0.9999 - val_loss: 1.6820e-10 - val_accuracy: 1.0000\n",
      "Epoch 454/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.9887e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 455/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.5064e-06 - accuracy: 1.0000 - val_loss: 3.3639e-10 - val_accuracy: 1.0000\n",
      "Epoch 456/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.1791e-06 - accuracy: 1.0000 - val_loss: 1.6820e-10 - val_accuracy: 1.0000\n",
      "Epoch 457/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.4141e-05 - accuracy: 1.0000 - val_loss: 4.7095e-09 - val_accuracy: 1.0000\n",
      "Epoch 458/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.1210e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 459/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.3082e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 460/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0835e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 461/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.8026e-05 - accuracy: 0.9999 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 462/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.3309e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 463/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.6731e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 464/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.6916e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 465/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.5427e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 466/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.3122e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 467/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.2576e-06 - accuracy: 1.0000 - val_loss: 2.1025e-10 - val_accuracy: 1.0000\n",
      "Epoch 468/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.1309e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.0374e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 470/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.1695e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 471/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.4430e-06 - accuracy: 1.0000 - val_loss: 1.6820e-10 - val_accuracy: 1.0000\n",
      "Epoch 472/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.2438e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 473/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.4873e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 474/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.0777e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 475/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.4621e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 476/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.2438e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 477/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1334e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 478/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 9.3169e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 479/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.5022e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 480/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.4664e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 481/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.1451e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 482/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 6.6961e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 483/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.0815e-04 - accuracy: 0.9999 - val_loss: 4.6254e-10 - val_accuracy: 1.0000\n",
      "Epoch 484/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.6052e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 485/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.7769e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 486/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1016e-04 - accuracy: 0.9999 - val_loss: 2.5229e-10 - val_accuracy: 1.0000\n",
      "Epoch 487/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 3.3073e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 488/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 8.3557e-06 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 489/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.7420e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 490/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.2692e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 491/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.2314e-05 - accuracy: 1.0000 - val_loss: 1.2615e-10 - val_accuracy: 1.0000\n",
      "Epoch 492/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.1237e-04 - accuracy: 0.9999 - val_loss: 5.5504e-09 - val_accuracy: 1.0000\n",
      "Epoch 493/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 1.5470e-04 - accuracy: 0.9999 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 494/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 4.3793e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 495/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.9326e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 496/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.4743e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 497/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 7.3259e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 498/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 2.7437e-05 - accuracy: 1.0000 - val_loss: 4.2049e-11 - val_accuracy: 1.0000\n",
      "Epoch 499/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.6889e-06 - accuracy: 1.0000 - val_loss: 8.4098e-11 - val_accuracy: 1.0000\n",
      "Epoch 500/500\n",
      "11336/11336 [==============================] - 16s 1ms/step - loss: 5.3236e-05 - accuracy: 1.0000 - val_loss: 1.6820e-10 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe46f3a7b10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
    }
   ],
   "source": [
    "# Fit the NN\n",
    "batch_size = 120\n",
    "epochs = 500\n",
    "\n",
    "model2.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to models directory.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# serialize model to JSON\n",
    "model_json = model2.to_json()\n",
    "with open(\"../output/models/sequential_500epochs.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model2.save_weights(\"../output/models/sequential_500epochs.h5\")\n",
    "print(\"Saved model to models directory.\")'''\n",
    "\n",
    "# Guardar arquitectura + pesos en un solo archivo HDF5\n",
    "\n",
    "model2.save(\"../output/models/sequential_500epochs.json\")\n",
    "model.save(\"../output/models/sequential_30epochs.json\")\n",
    "print(\"Saved model to models directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probando el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict y predict_proba me devuelve el mismo array."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 396,
=======
   "execution_count": 148,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test_vector)\n",
    "#print(\"Accuracy score: \", accuracy_score(y_test, y_pred, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 397,
=======
   "execution_count": 150,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = np.argmax(y_pred, axis=1)\n",
    "reality = np.argmax(y_test_vector, axis=1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 398,
=======
   "execution_count": 151,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score: \", accuracy_score(reality, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[865]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5954979e-13, 1.6716356e-13, 1.0000000e+00, 9.3996338e-12,\n",
       "       4.4506796e-16, 3.8997217e-14, 7.6325139e-15], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[865]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy de 1... Demasiado locura.\n",
    "#### Pruebo modelo con muestra fuera del dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 169,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [],
   "source": [
    "from resizeimage import resizeimage\n",
    "\n",
    "def tryImage(path):\n",
    "    with open(path, 'r+b') as f:\n",
    "        with Image.open(f) as image:\n",
    "            cover = resizeimage.resize_cover(image, [28, 28])\n",
    "            img = cover.convert('L')\n",
    "            arr = np.array(img).flatten().reshape((1,28,28,1))\n",
<<<<<<< HEAD
    "            return arr\n",
    "\n",
    "\n",
    "def plotImage(path):\n",
    "    with open(path, 'r+b') as f:\n",
    "        with Image.open(f) as image:\n",
    "            cover = resizeimage.resize_cover(image, [28, 28])\n",
    "            img = cover.convert('L')\n",
    "            return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plotImage('pruebas/prueba5.jpg')\n",
    "arr = tryImage('pruebas/prueba5.jpg')"
=======
    "            return arr\n"
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 483,
=======
   "execution_count": 159,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "<matplotlib.image.AxesImage at 0x7fe47ce4c910>"
      ]
     },
     "execution_count": 483,
=======
       "<matplotlib.image.AxesImage at 0x7fe4825edb50>"
      ]
     },
     "execution_count": 159,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
<<<<<<< HEAD
      "image/png": "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\n",
=======
      "image/png": "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\n",
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
<<<<<<< HEAD
    "plt.imshow(np.asarray(X_test.loc[14055]).reshape((28,28)))"
=======
    "plt.imshow(img, cmap=\"gray\")"
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe47ce69710>"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
=======
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 28, 28, 1) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-75671dfcc88e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprueba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtryImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pruebas/prueba3.jpeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprueba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2675\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2677\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2678\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2679\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1597\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5677\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5679\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5680\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    688\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    689\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 690\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 28, 28, 1) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
<<<<<<< HEAD
    "plt.imshow(arr.reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_vector[0]\n",
    "\n"
=======
    "prueba = tryImage('pruebas/prueba3.jpeg')\n",
    "plt.imshow(prueba, cmap=\"gray\")"
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 480,
=======
   "execution_count": 183,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "array([[1., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 480,
=======
       "array([[0., 0., 0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 183,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "model.predict(arr)"
=======
    "model.predict(prueba)"
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 464,
=======
   "execution_count": 176,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>F</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>Y</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <th>6648</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6407</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6698</th>\n",
=======
       "      <th>3448</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13025</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9002</th>\n",
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
<<<<<<< HEAD
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6355</th>\n",
=======
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12540</th>\n",
       "      <td>1</td>\n",
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
<<<<<<< HEAD
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13930</th>\n",
=======
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10298</th>\n",
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
<<<<<<< HEAD
       "      <td>1</td>\n",
       "      <td>0</td>\n",
=======
       "      <td>0</td>\n",
       "      <td>1</td>\n",
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <th>13864</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5769</th>\n",
=======
       "      <th>4058</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1</td>\n",
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
<<<<<<< HEAD
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6017</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14055</th>\n",
       "      <td>0</td>\n",
=======
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4442</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
<<<<<<< HEAD
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5921</th>\n",
=======
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8817</th>\n",
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
<<<<<<< HEAD
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows Ã— 7 columns</p>\n",
=======
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2835 rows Ã— 7 columns</p>\n",
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
       "</div>"
      ],
      "text/plain": [
       "       A  B  F  T  V  Y  other\n",
<<<<<<< HEAD
       "6648   0  0  0  0  0  1      0\n",
       "6407   0  0  0  0  0  1      0\n",
       "6698   0  0  0  0  0  1      0\n",
       "6355   0  0  0  0  0  1      0\n",
       "13930  0  0  0  0  0  1      0\n",
       "...   .. .. .. .. .. ..    ...\n",
       "13864  0  0  0  0  0  1      0\n",
       "5769   0  0  0  0  0  1      0\n",
       "6017   0  0  0  0  0  1      0\n",
       "14055  0  0  0  0  0  1      0\n",
       "5921   0  0  0  0  0  1      0\n",
       "\n",
       "[272 rows x 7 columns]"
      ]
     },
     "execution_count": 464,
=======
       "3448   0  0  0  1  0  0      0\n",
       "13025  0  0  1  0  0  0      0\n",
       "9002   0  0  0  0  0  0      1\n",
       "12540  1  0  0  0  0  0      0\n",
       "10298  0  0  0  0  0  0      1\n",
       "...   .. .. .. .. .. ..    ...\n",
       "4058   0  0  0  1  0  0      0\n",
       "85     1  0  0  0  0  0      0\n",
       "4442   0  0  0  1  0  0      0\n",
       "1219   0  1  0  0  0  0      0\n",
       "8817   0  0  0  0  0  0      1\n",
       "\n",
       "[2835 rows x 7 columns]"
      ]
     },
     "execution_count": 176,
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "y_test[y_test.Y == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo con mucho mÃ¡s ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/processed/noisyData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"label\", axis=1)\n",
    "y = data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25907, 28, 28, 1)\n",
      "25907 train samples\n",
      "6477 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 7\n",
    "batch_size = 128\n",
    "epochs = 12\n",
    "\n",
    "# DF to np array. Keras needs one-hot encoded y for multilabel classification.\n",
    "\n",
    "X_train_vector = X_train.values.reshape((X_train.shape[0], img_rows, img_cols))\n",
    "X_test_vector = X_test.values.reshape((X_test.shape[0], img_rows, img_cols))\n",
    "\n",
    "\n",
    "# Ask keras which format to use depending on used backend and arrange data as expected\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train_vector = X_train_vector.reshape(X_train_vector.shape[0], 1, img_rows, img_cols)\n",
    "    X_test_vector = X_test_vector.reshape(X_test_vector.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train_vector = X_train_vector.reshape(X_train_vector.shape[0], img_rows, img_cols, 1)\n",
    "    X_test_vector = X_test_vector.reshape(X_test_vector.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "'''# Incoming data is in uint8. Cast the input data images to be floats in range [0.0-1.0]  \n",
    "X_train_vector = X_train_vector.astype('float32') / 255\n",
    "X_test_vector = X_test_vector.astype('float32') / 255'''\n",
    "\n",
    "print('x_train shape:', X_train_vector.shape)\n",
    "print(X_train_vector.shape[0], 'train samples')\n",
    "print(X_test_vector.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to class matrices\n",
    "y_train_vector = y_train.values\n",
    "y_test_vector = y_test.values\n",
    "\n",
    "## This is the neural network proposed architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25907 samples, validate on 6477 samples\n",
      "Epoch 1/5\n",
      "25907/25907 [==============================] - 63s 2ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 1.6981e-07 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "25907/25907 [==============================] - 63s 2ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 8.9426e-08 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "25907/25907 [==============================] - 64s 2ms/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 3.8052e-07 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "25907/25907 [==============================] - 64s 2ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 2.0901e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "25907/25907 [==============================] - 64s 2ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 3.5374e-08 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe482077d10>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the NN\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train_vector, y_train_vector,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_vector, y_test_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.5373611647829083e-08\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model with test data\n",
    "score = model.evaluate(X_test_vector, y_test_vector, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save(\"../output/models/noisyData_5epochs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  1.0\n",
      "F score:  [1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = model.predict(X_test_vector)\n",
    "predict = np.argmax(y_pred, axis=1)\n",
    "reality = np.argmax(y_test_vector, axis=1)\n",
    "print(\"Accuracy score: \", accuracy_score(reality, predict))\n",
    "print(\"F score: \", f1_score(reality, predict, average=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.3720337e-33, 0.0000000e+00, 1.0000000e+00], dtype=float32),\n",
       " array([0, 0, 0, 0, 0, 0, 1], dtype=uint8))"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[865], y_test_vector[865]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plotImage('pruebas/prueba3.jpeg')\n",
    "arr = tryImage('pruebas/prueba3.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.4494786e-31,\n",
       "        5.3927119e-20, 6.2993355e-27, 1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>F</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>Y</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26299</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32050</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11176</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14623</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17329</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30725</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6477 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       A  B  F  T  V  Y  other\n",
       "9997   0  0  0  0  0  0      1\n",
       "26299  0  0  0  0  1  0      0\n",
       "3053   0  0  0  0  0  0      1\n",
       "1815   0  0  0  0  0  0      1\n",
       "32050  0  0  0  0  0  0      1\n",
       "...   .. .. .. .. .. ..    ...\n",
       "11176  0  0  0  0  0  0      1\n",
       "14623  0  0  0  0  0  0      1\n",
       "4455   0  0  0  0  0  0      1\n",
       "17329  0  0  0  0  0  0      1\n",
       "30725  0  0  0  0  0  0      1\n",
       "\n",
       "[6477 rows x 7 columns]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test# Evaluate the model with test data\n",
    "score = model.evaluate(X_test_vector, y_test_vector, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save(\"../output/models/noisyData_5epochs.json\")# Evaluate the model with test data\n",
    "score = model.evaluate(X_test_vector, y_test_vector, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save(\"../output/models/noisyData_5epochs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo con ruido balanceado y mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/processed/data_2# Fit the NN\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train_vector, y_train_vector,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_vector, y_test_vector))# Evaluate the model with test data\n",
    "score = model.evaluate(X_test_vector, y_test_vector, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save(\"../output/models/noisyData_5epochs.json\").csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"label\", axis=1)\n",
    "y = data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (13489, 28, 28, 1)\n",
      "13489 train samples\n",
      "3373 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 7\n",
    "batch_size = 128\n",
    "epochs = 12\n",
    "\n",
    "# DF to np array. Keras needs one-hot encoded y for multilabel classification.\n",
    "\n",
    "X_train_vector = X_train.values.reshape((X_train.shape[0], img_rows, img_cols))\n",
    "X_test_vector = X_test.values.reshape((X_test.shape[0], img_rows, img_cols))\n",
    "\n",
    "\n",
    "# Ask keras which format to use depending on used backend and arrange data as expected\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train_vector = X_train_vector.reshape(X_train_vector.shape[0], 1, img_rows, img_cols)\n",
    "    X_test_vector = X_test_vector.reshape(X_test_vector.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train_vector = X_train_vector.reshape(X_train_vector.shape[0], img_rows, img_cols, 1)\n",
    "    X_test_vector = X_test_vector.reshape(X_test_vector.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "print('x_train shape:', X_train_vector.shape)\n",
    "print(X_train_vector.shape[0], 'train samples')\n",
    "print(X_test_vector.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to class matrices\n",
    "y_train_vector = y_train.values\n",
    "y_test_vector = y_test.values\n",
    "\n",
    "## This is the neural network proposed architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model# Evaluate the model with test data\n",
    "score = model.evaluate(X_test_vector, y_test_vector, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save(\"../output/models/noisyData_5epochs.json\").add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13489 samples, validate on 3373 samples\n",
      "Epoch 1/5\n",
      "13489/13489 [==============================] - 31s 2ms/step - loss: 1.0761 - accuracy: 0.8529 - val_loss: 0.0765 - val_accuracy: 0.9733\n",
      "Epoch 2/5\n",
      "13489/13489 [==============================] - 29s 2ms/step - loss: 0.0954 - accuracy: 0.9732 - val_loss: 0.0102 - val_accuracy: 0.9970\n",
      "Epoch 3/5\n",
      "13489/13489 [==============================] - 28s 2ms/step - loss: 0.0414 - accuracy: 0.9877 - val_loss: 0.0162 - val_accuracy: 0.9953\n",
      "Epoch 4/5\n",
      "13489/13489 [==============================] - 29s 2ms/step - loss: 0.0214 - accuracy: 0.9926 - val_loss: 0.0055 - val_accuracy: 0.9982\n",
      "Epoch 5/5\n",
      "13489/13489 [==============================] - 28s 2ms/step - loss: 0.0224 - accuracy: 0.9944 - val_loss: 0.0013 - val_accuracy: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe482a74450>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the NN\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train_vector, y_train_vector,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_vector, y_test_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0012793392593377507\n",
      "Test accuracy: 0.9994070529937744\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model with test data\n",
    "score = model.evaluate(X_test_vector, y_test_vector, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save(\"../output/models/data_2_5epochs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9994070560332049\n",
      "F score:  [0.99828473 1.         1.         1.         0.99829642 1.\n",
      " 0.99938462]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = model.predict(X_test_vector)\n",
    "predict = np.argmax(y_pred, axis=1)\n",
    "reality = np.argmax(y_test_vector, axis=1)\n",
    "print(\"Accuracy score: \", accuracy_score(reality, predict))\n",
    "print(\"F score: \", f1_score(reality, predict, average=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.2759675e-17, 9.9177769e-26, 1.0642394e-21, 2.6498999e-22,\n",
       "        1.0329587e-18, 4.7052531e-22, 1.0000000e+00], dtype=float32),\n",
       " array([0, 0, 0, 0, 0, 0, 1], dtype=uint8))"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[865], y_test_vector[865]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plotImage('pruebas/prueba1.jpg')\n",
    "arr = tryImage('pruebas/prueba1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9456358e-07, 9.2395531e-08, 6.7506399e-08, 1.5329319e-04,\n",
       "        2.7089988e-04, 7.8341151e-03, 9.9174130e-01]], dtype=float32)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation con este modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (13489, 28, 28, 1)\n",
      "13489 train samples\n",
      "3373 test samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# convert class vectors to class matrices\\ny_vector = pd.get_dummies(y)\\ny_vector = y_vector.values'"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# DF to np array. Keras needs one-hot encoded y for multilabel classification.\n",
    "\n",
    "X_vector = X.values.reshape((X.shape[0], img_rows, img_cols))\n",
    "\n",
    "\n",
    "# Ask keras which format to use depending on used backend and arrange data as expected\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_vector = X_vector.reshape(X_vector.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_vector = X_vector.reshape(X_vector.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "'''# Incoming data is in uint8. Cast the input data images to be floats in range [0.0-1.0]  \n",
    "X_train_vector = X_train_vector.astype('float32') / 255\n",
    "X_test_vector = X_test_vector.astype('float32') / 255'''\n",
    "\n",
    "print('x_train shape:', X_train_vector.shape)\n",
    "print(X_train_vector.shape[0], 'train samples')\n",
    "print(X_test_vector.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to class matrices\n",
    "label_encoder = LabelEncoder()\n",
    "y_vector = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13489/13489 [==============================] - 10s 753us/step - loss: 3.4617 - accuracy: 0.4646\n",
      "Epoch 2/10\n",
      "13489/13489 [==============================] - 10s 736us/step - loss: 1.3788 - accuracy: 0.4862\n",
      "Epoch 3/10\n",
      "13489/13489 [==============================] - 10s 751us/step - loss: 0.8520 - accuracy: 0.6753\n",
      "Epoch 4/10\n",
      "13489/13489 [==============================] - 10s 747us/step - loss: 0.2355 - accuracy: 0.9190\n",
      "Epoch 5/10\n",
      "13489/13489 [==============================] - 10s 733us/step - loss: 0.1177 - accuracy: 0.9581\n",
      "Epoch 6/10\n",
      "13489/13489 [==============================] - 10s 710us/step - loss: 0.0767 - accuracy: 0.9749\n",
      "Epoch 7/10\n",
      "13489/13489 [==============================] - 10s 720us/step - loss: 0.0550 - accuracy: 0.9835\n",
      "Epoch 8/10\n",
      "13489/13489 [==============================] - 10s 721us/step - loss: 0.0439 - accuracy: 0.9861\n",
      "Epoch 9/10\n",
      "13489/13489 [==============================] - 10s 717us/step - loss: 0.0373 - accuracy: 0.9884\n",
      "Epoch 10/10\n",
      "13489/13489 [==============================] - 10s 725us/step - loss: 0.0314 - accuracy: 0.9904\n",
      "3373/3373 [==============================] - 0s 137us/step\n",
      "Epoch 1/10\n",
      "13489/13489 [==============================] - 10s 751us/step - loss: 4.0539 - accuracy: 0.4642\n",
      "Epoch 2/10\n",
      "13489/13489 [==============================] - 11s 800us/step - loss: 1.2607 - accuracy: 0.5147\n",
      "Epoch 3/10\n",
      "13489/13489 [==============================] - 11s 782us/step - loss: 0.3365 - accuracy: 0.8798\n",
      "Epoch 4/10\n",
      "13489/13489 [==============================] - 10s 736us/step - loss: 0.1358 - accuracy: 0.9520\n",
      "Epoch 5/10\n",
      "13489/13489 [==============================] - 10s 732us/step - loss: 0.0912 - accuracy: 0.9703\n",
      "Epoch 6/10\n",
      "13489/13489 [==============================] - 13s 975us/step - loss: 0.0649 - accuracy: 0.9802\n",
      "Epoch 7/10\n",
      "13489/13489 [==============================] - 14s 1ms/step - loss: 0.0539 - accuracy: 0.9833\n",
      "Epoch 8/10\n",
      "13489/13489 [==============================] - 12s 876us/step - loss: 0.0409 - accuracy: 0.9873\n",
      "Epoch 9/10\n",
      "13489/13489 [==============================] - 10s 770us/step - loss: 0.0296 - accuracy: 0.9897\n",
      "Epoch 10/10\n",
      "13489/13489 [==============================] - 13s 939us/step - loss: 0.0374 - accuracy: 0.9875\n",
      "3373/3373 [==============================] - 1s 256us/step\n",
      "Epoch 1/10\n",
      "13490/13490 [==============================] - 8s 617us/step - loss: 3.2485 - accuracy: 0.4678\n",
      "Epoch 2/10\n",
      "13490/13490 [==============================] - 8s 573us/step - loss: 1.4000 - accuracy: 0.4862\n",
      "Epoch 3/10\n",
      "13490/13490 [==============================] - 8s 564us/step - loss: 0.5461 - accuracy: 0.8016\n",
      "Epoch 4/10\n",
      "13490/13490 [==============================] - 10s 757us/step - loss: 0.1479 - accuracy: 0.9523\n",
      "Epoch 5/10\n",
      "13490/13490 [==============================] - 10s 721us/step - loss: 0.0728 - accuracy: 0.9762\n",
      "Epoch 6/10\n",
      "13490/13490 [==============================] - 10s 748us/step - loss: 0.0621 - accuracy: 0.9808\n",
      "Epoch 7/10\n",
      "13490/13490 [==============================] - 10s 731us/step - loss: 0.0398 - accuracy: 0.9867\n",
      "Epoch 8/10\n",
      "13490/13490 [==============================] - 10s 745us/step - loss: 0.0333 - accuracy: 0.9893\n",
      "Epoch 9/10\n",
      "13490/13490 [==============================] - 10s 738us/step - loss: 0.0330 - accuracy: 0.9899\n",
      "Epoch 10/10\n",
      "13490/13490 [==============================] - 10s 752us/step - loss: 0.0220 - accuracy: 0.9936\n",
      "3372/3372 [==============================] - 0s 144us/step\n",
      "Epoch 1/10\n",
      "13490/13490 [==============================] - 10s 743us/step - loss: 3.8586 - accuracy: 0.4669\n",
      "Epoch 2/10\n",
      "13490/13490 [==============================] - 10s 757us/step - loss: 1.4976 - accuracy: 0.4855\n",
      "Epoch 3/10\n",
      "13490/13490 [==============================] - 10s 742us/step - loss: 0.7846 - accuracy: 0.7073\n",
      "Epoch 4/10\n",
      "13490/13490 [==============================] - 10s 760us/step - loss: 0.1831 - accuracy: 0.9370\n",
      "Epoch 5/10\n",
      "13490/13490 [==============================] - 10s 757us/step - loss: 0.0890 - accuracy: 0.9715\n",
      "Epoch 6/10\n",
      "13490/13490 [==============================] - 10s 755us/step - loss: 0.0559 - accuracy: 0.9809\n",
      "Epoch 7/10\n",
      "13490/13490 [==============================] - 10s 761us/step - loss: 0.0582 - accuracy: 0.9830\n",
      "Epoch 8/10\n",
      "13490/13490 [==============================] - 10s 748us/step - loss: 0.0340 - accuracy: 0.9886\n",
      "Epoch 9/10\n",
      "13490/13490 [==============================] - 10s 743us/step - loss: 0.0275 - accuracy: 0.9920\n",
      "Epoch 10/10\n",
      "13490/13490 [==============================] - 10s 763us/step - loss: 0.0265 - accuracy: 0.9917\n",
      "3372/3372 [==============================] - 0s 144us/step\n",
      "Epoch 1/10\n",
      "13490/13490 [==============================] - 13s 940us/step - loss: 3.2377 - accuracy: 0.4701\n",
      "Epoch 2/10\n",
      "13490/13490 [==============================] - 11s 844us/step - loss: 1.2866 - accuracy: 0.5423\n",
      "Epoch 3/10\n",
      "13490/13490 [==============================] - 11s 787us/step - loss: 0.3138 - accuracy: 0.8883\n",
      "Epoch 4/10\n",
      "13490/13490 [==============================] - 10s 744us/step - loss: 0.1259 - accuracy: 0.9577\n",
      "Epoch 5/10\n",
      "13490/13490 [==============================] - 11s 783us/step - loss: 0.0766 - accuracy: 0.9755\n",
      "Epoch 6/10\n",
      "13490/13490 [==============================] - 10s 757us/step - loss: 0.0589 - accuracy: 0.9816\n",
      "Epoch 7/10\n",
      "13490/13490 [==============================] - 11s 847us/step - loss: 0.0457 - accuracy: 0.9847\n",
      "Epoch 8/10\n",
      "13490/13490 [==============================] - 11s 821us/step - loss: 0.0395 - accuracy: 0.9881\n",
      "Epoch 9/10\n",
      "13490/13490 [==============================] - 11s 793us/step - loss: 0.0343 - accuracy: 0.9893\n",
      "Epoch 10/10\n",
      "13490/13490 [==============================] - 10s 756us/step - loss: 0.0240 - accuracy: 0.9931\n",
      "3372/3372 [==============================] - 0s 144us/step\n",
      "Baseline: 99.90% (0.04%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def create_baseline():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "\n",
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=10, batch_size=32, verbose=1)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "results = cross_val_score(estimator, X_vector, y_vector, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../output/models/data_2_5KFold.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.6472950e-29,\n",
       "        9.4630553e-36, 4.3700610e-23, 3.4564358e-38]], dtype=float32)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = plotImage('pruebas/prueba1.jpg')\n",
    "arr = tryImage('pruebas/prueba4.jpeg')\n",
    "\n",
    "model.predict(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba con nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11336 samples, validate on 2835 samples\n",
      "Epoch 1/5\n",
      "11336/11336 [==============================] - 26s 2ms/step - loss: 0.0930 - accuracy: 0.9772 - val_loss: 0.0017 - val_accuracy: 0.9996\n",
      "Epoch 2/5\n",
      "11336/11336 [==============================] - 25s 2ms/step - loss: 0.0142 - accuracy: 0.9967 - val_loss: 1.2297e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "11336/11336 [==============================] - 26s 2ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 9.0628e-06 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "11336/11336 [==============================] - 25s 2ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 5.0692e-06 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "11336/11336 [==============================] - 26s 2ms/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 2.9463e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb5e4126850>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construction of model\n",
    "model = Sequential()\n",
    "chanDim = -1\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "    chanDim = 1\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=input_shape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fit the NN\n",
    "batch_size = 8\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train_vector, y_train_vector,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_vector, y_test_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.00029463485427516677\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_vector, y_test_vector, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"../output/models/deeper_5epochs.json\")\n",
    "arr = tryImage('pruebas/prueba3.jpeg')\n",
    "model.predict(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>F</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>Y</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13025</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9002</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12540</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10298</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4058</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4442</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8817</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2835 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       A  B  F  T  V  Y  other\n",
       "3448   0  0  0  1  0  0      0\n",
       "13025  0  0  1  0  0  0      0\n",
       "9002   0  0  0  0  0  0      1\n",
       "12540  1  0  0  0  0  0      0\n",
       "10298  0  0  0  0  0  0      1\n",
       "...   .. .. .. .. .. ..    ...\n",
       "4058   0  0  0  1  0  0      0\n",
       "85     1  0  0  0  0  0      0\n",
       "4442   0  0  0  1  0  0      0\n",
       "1219   0  1  0  0  0  0      0\n",
       "8817   0  0  0  0  0  0      1\n",
       "\n",
       "[2835 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA del dataset inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>...</td>\n",
       "      <td>179.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>198.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>231.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>147.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>...</td>\n",
       "      <td>165.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0   197.0   195.0   196.0   195.0   197.0   196.0   195.0   196.0   196.0   \n",
       "1   142.0   144.0   144.0   146.0   147.0   149.0   150.0   151.0   153.0   \n",
       "2   198.0   200.0   201.0   200.0   199.0   198.0   198.0   197.0   198.0   \n",
       "3   231.0   232.0   234.0   237.0   238.0   241.0   243.0   244.0   248.0   \n",
       "4   147.0   149.0   150.0   152.0   153.0   153.0   152.0   153.0   154.0   \n",
       "\n",
       "   pixel10  ...  pixel776  pixel777  pixel778  pixel779  pixel780  pixel781  \\\n",
       "0    196.0  ...      65.0     182.0     213.0     211.0     212.0     212.0   \n",
       "1    154.0  ...     179.0     179.0     180.0     181.0     182.0     182.0   \n",
       "2    199.0  ...      99.0      99.0      98.0      99.0      98.0     100.0   \n",
       "3    249.0  ...      66.0     199.0     255.0     255.0     255.0     255.0   \n",
       "4    154.0  ...     165.0     166.0     165.0     166.0     169.0     167.0   \n",
       "\n",
       "   pixel782  pixel783  pixel784  label  \n",
       "0     213.0     213.0     213.0      A  \n",
       "1     182.0     183.0     183.0      A  \n",
       "2     100.0     101.0     100.0      A  \n",
       "3     255.0     255.0     255.0      A  \n",
       "4     133.0     135.0     140.0      A  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (11336, 50)\n",
      "11336 train samples\n",
      "2835 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 7\n",
    "\n",
    "# DF to np array. Keras needs one-hot encoded y for multilabel classification.\n",
    "\n",
    "X_train_vector = X_train_pca.reshape((X_train_pca.shape[0], X_train_pca.shape[1]))\n",
    "X_test_vector = X_test_pca.reshape((X_test_pca.shape[0], X_train_pca.shape[1]))\n",
    "\n",
    "'''\n",
    "# Ask keras which format to use depending on used backend and arrange data as expected\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train_vector = X_train_vector.reshape(X_train_vector.shape[0], 1, img_rows, img_cols)\n",
    "    X_test_vector = X_test_vector.reshape(X_test_vector.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train_vector = X_train_vector.reshape(X_train_vector.shape[0], img_rows, img_cols, 1)\n",
    "    X_test_vector = X_test_vector.reshape(X_test_vector.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Incoming data is in uint8. Cast the input data images to be floats in range [0.0-1.0]  \n",
    "X_train_vector = X_train_vector.astype('float32') / 255\n",
    "X_test_vector = X_test_vector.astype('float32') / 255'''\n",
    "\n",
    "print('X_train shape:', X_train_vector.shape)\n",
    "print(X_train_vector.shape[0], 'train samples')\n",
    "print(X_test_vector.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to class matrices\n",
    "y_train_vector = y_train.values\n",
    "y_test_vector = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the neural network proposed architecture\n",
    "input_shape = (X_train_vector.shape[0], X_train_vector.shape[1], 1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the NN\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train_vector, y_train_vector,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_vector, y_test_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11336, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NasNET - Input Shape tiene que ser al menos 32,32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"NASNet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "stem_conv1 (Conv2D)             (None, 15, 15, 32)   288         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "stem_bn1 (BatchNormalization)   (None, 15, 15, 32)   128         stem_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 15, 32)   0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_stem_1 (Conv2D (None, 15, 15, 11)   352         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_stem_1 (BatchNor (None, 15, 15, 11)   44          reduction_conv_1_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 15, 11)   0           reduction_bn_1_stem_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 15, 15, 32)   0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 19, 19, 11)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 21, 21, 32)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 8, 8, 11)     396         separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 8, 8, 11)     1920        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 8, 8, 11)     44          separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 8, 8, 11)     44          separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 8, 8, 11)     0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 8, 11)     0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 8, 8, 11)     396         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 8, 8, 11)     660         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 15, 15, 32)   0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 8, 8, 11)     44          separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 8, 8, 11)     44          separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 21, 21, 32)   0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 15, 15, 32)   0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_stem_1 (Add)    (None, 8, 8, 11)     0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 8, 8, 11)     1920        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 19, 19, 32)   0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 8, 11)     0           reduction_add_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 8, 8, 11)     44          separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 8, 8, 11)     1152        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 8, 8, 11)     220         activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 8, 8, 11)     0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 8, 8, 11)     44          separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 8, 8, 11)     44          separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_stem_1 (ZeroPad (None, 17, 17, 11)   0           reduction_bn_1_stem_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 8, 8, 11)     660         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 11)     0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 8, 8, 11)     0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_stem_1 (MaxPool (None, 8, 8, 11)     0           reduction_pad_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 8, 8, 11)     44          separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 8, 8, 11)     396         activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 8, 8, 11)     220         activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_stem_2 (Activatio (None, 15, 15, 32)   0           stem_bn1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_stem_1 (Add)    (None, 8, 8, 11)     0           reduction_left2_stem_1[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_stem_1 (Average (None, 8, 8, 11)     0           reduction_pad_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 8, 8, 11)     44          separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_stem_1 (Average (None, 8, 8, 11)     0           reduction_add_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 8, 8, 11)     44          separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_stem_1 (MaxPoo (None, 8, 8, 11)     0           reduction_pad_1_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 16, 16, 32)   0           adjust_relu_1_stem_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_stem_1 (Add)     (None, 8, 8, 11)     0           reduction_left3_stem_1[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 8, 8, 11)     0           reduction_add_2_stem_1[0][0]     \n",
      "                                                                 reduction_left4_stem_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_stem_1 (Add)     (None, 8, 8, 11)     0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)       (None, 15, 15, 32)   0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_stem_1 (Concat (None, 8, 8, 44)     0           reduction_add_2_stem_1[0][0]     \n",
      "                                                                 reduction_add3_stem_1[0][0]      \n",
      "                                                                 add_1[0][0]                      \n",
      "                                                                 reduction_add4_stem_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_stem_2 (Avera (None, 8, 8, 32)     0           adjust_relu_1_stem_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_stem_2 (Avera (None, 8, 8, 32)     0           cropping2d_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 44)     0           reduction_concat_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_stem_2 (Conv2D)   (None, 8, 8, 11)     352         adjust_avg_pool_1_stem_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_stem_2 (Conv2D)   (None, 8, 8, 11)     352         adjust_avg_pool_2_stem_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_stem_2 (Conv2D (None, 8, 8, 22)     968         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 22)     0           adjust_conv_1_stem_2[0][0]       \n",
      "                                                                 adjust_conv_2_stem_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_stem_2 (BatchNor (None, 8, 8, 22)     88          reduction_conv_1_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_stem_2 (BatchNormaliz (None, 8, 8, 22)     88          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 22)     0           reduction_bn_1_stem_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 22)     0           adjust_bn_stem_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 11, 11, 22)   0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 13, 13, 22)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 4, 4, 22)     1034        separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 4, 4, 22)     1562        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 4, 4, 22)     88          separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 4, 4, 22)     88          separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 4, 4, 22)     0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 4, 4, 22)     0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 4, 4, 22)     1034        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 4, 4, 22)     1562        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 22)     0           adjust_bn_stem_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 4, 4, 22)     88          separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 4, 4, 22)     88          separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 13, 13, 22)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 22)     0           adjust_bn_stem_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_stem_2 (Add)    (None, 4, 4, 22)     0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 4, 4, 22)     1562        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 11, 11, 22)   0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 4, 4, 22)     0           reduction_add_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 4, 4, 22)     88          separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 4, 4, 22)     1034        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 4, 4, 22)     682         activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 4, 4, 22)     0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 4, 4, 22)     88          separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 4, 4, 22)     88          separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_stem_2 (ZeroPad (None, 9, 9, 22)     0           reduction_bn_1_stem_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 4, 4, 22)     1562        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 4, 4, 22)     0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 4, 4, 22)     0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_stem_2 (MaxPool (None, 4, 4, 22)     0           reduction_pad_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 4, 4, 22)     88          separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 4, 4, 22)     1034        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 4, 4, 22)     682         activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_0 (Activation)    (None, 8, 8, 44)     0           reduction_concat_stem_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_stem_2 (Add)    (None, 4, 4, 22)     0           reduction_left2_stem_2[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_stem_2 (Average (None, 4, 4, 22)     0           reduction_pad_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 4, 4, 22)     88          separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_stem_2 (Average (None, 4, 4, 22)     0           reduction_add_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 4, 4, 22)     88          separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_stem_2 (MaxPoo (None, 4, 4, 22)     0           reduction_pad_1_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 9, 9, 44)     0           adjust_relu_1_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_stem_2 (Add)     (None, 4, 4, 22)     0           reduction_left3_stem_2[0][0]     \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 4, 4, 22)     0           reduction_add_2_stem_2[0][0]     \n",
      "                                                                 reduction_left4_stem_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_stem_2 (Add)     (None, 4, 4, 22)     0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_2 (Cropping2D)       (None, 8, 8, 44)     0           zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_stem_2 (Concat (None, 4, 4, 88)     0           reduction_add_2_stem_2[0][0]     \n",
      "                                                                 reduction_add3_stem_2[0][0]      \n",
      "                                                                 add_2[0][0]                      \n",
      "                                                                 reduction_add4_stem_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_0 (AveragePoo (None, 4, 4, 44)     0           adjust_relu_1_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_0 (AveragePoo (None, 4, 4, 44)     0           cropping2d_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_0 (Conv2D)        (None, 4, 4, 22)     968         adjust_avg_pool_1_0[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_0 (Conv2D)        (None, 4, 4, 22)     968         adjust_avg_pool_2_0[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 4, 4, 88)     0           reduction_concat_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 4, 4, 44)     0           adjust_conv_1_0[0][0]            \n",
      "                                                                 adjust_conv_2_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_0 (Conv2D)        (None, 4, 4, 44)     3872        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_0 (BatchNormalization (None, 4, 4, 44)     176         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_0 (BatchNormalizati (None, 4, 4, 44)     176         normal_conv_1_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 4, 4, 44)     0           normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 4, 4, 44)     0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 4, 4, 44)     0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 4, 4, 44)     0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 4, 4, 44)     0           normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_0 (None, 4, 4, 44)     3036        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 4, 4, 44)     2332        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_0 (None, 4, 4, 44)     3036        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 4, 4, 44)     2332        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_0 (None, 4, 4, 44)     2332        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 4, 4, 44)     176         separable_conv_1_normal_left1_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_1_normal_right1_0[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 4, 4, 44)     176         separable_conv_1_normal_left2_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_1_normal_right2_0[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 4, 4, 44)     176         separable_conv_1_normal_left5_0[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_0 (None, 4, 4, 44)     3036        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 4, 4, 44)     2332        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_0 (None, 4, 4, 44)     3036        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 4, 4, 44)     2332        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_0 (None, 4, 4, 44)     2332        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 4, 4, 44)     176         separable_conv_2_normal_left1_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_2_normal_right1_0[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 4, 4, 44)     176         separable_conv_2_normal_left2_0[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_2_normal_right2_0[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_0 (AveragePooling2 (None, 4, 4, 44)     0           normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_0 (AveragePooling2 (None, 4, 4, 44)     0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_0 (AveragePooling (None, 4, 4, 44)     0           adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 4, 4, 44)     176         separable_conv_2_normal_left5_0[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_0 (Add)            (None, 4, 4, 44)     0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_0 (Add)            (None, 4, 4, 44)     0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_0 (Add)            (None, 4, 4, 44)     0           normal_left3_0[0][0]             \n",
      "                                                                 adjust_bn_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_0 (Add)            (None, 4, 4, 44)     0           normal_left4_0[0][0]             \n",
      "                                                                 normal_right4_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_0 (Add)            (None, 4, 4, 44)     0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_0 (Concatenate)   (None, 4, 4, 264)    0           adjust_bn_0[0][0]                \n",
      "                                                                 normal_add_1_0[0][0]             \n",
      "                                                                 normal_add_2_0[0][0]             \n",
      "                                                                 normal_add_3_0[0][0]             \n",
      "                                                                 normal_add_4_0[0][0]             \n",
      "                                                                 normal_add_5_0[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 4, 4, 88)     0           reduction_concat_stem_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 4, 4, 264)    0           normal_concat_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_1 (Conv2 (None, 4, 4, 44)     3872        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_1 (Conv2D)        (None, 4, 4, 44)     11616       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_1 (BatchNormalization (None, 4, 4, 44)     176         adjust_conv_projection_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_1 (BatchNormalizati (None, 4, 4, 44)     176         normal_conv_1_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 4, 4, 44)     0           normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 4, 4, 44)     0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 4, 4, 44)     0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 4, 44)     0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 44)     0           normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 4, 4, 44)     3036        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 4, 4, 44)     2332        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 4, 4, 44)     3036        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 4, 4, 44)     2332        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 4, 4, 44)     2332        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 4, 4, 44)     176         separable_conv_1_normal_left1_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_1_normal_right1_1[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 4, 4, 44)     176         separable_conv_1_normal_left2_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_1_normal_right2_1[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 4, 4, 44)     176         separable_conv_1_normal_left5_1[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 4, 4, 44)     3036        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 4, 4, 44)     2332        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 4, 4, 44)     3036        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 4, 4, 44)     2332        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 4, 4, 44)     2332        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 4, 4, 44)     176         separable_conv_2_normal_left1_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_2_normal_right1_1[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 4, 4, 44)     176         separable_conv_2_normal_left2_1[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_2_normal_right2_1[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_1 (AveragePooling2 (None, 4, 4, 44)     0           normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_1 (AveragePooling2 (None, 4, 4, 44)     0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_1 (AveragePooling (None, 4, 4, 44)     0           adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 4, 4, 44)     176         separable_conv_2_normal_left5_1[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_1 (Add)            (None, 4, 4, 44)     0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_1 (Add)            (None, 4, 4, 44)     0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_1 (Add)            (None, 4, 4, 44)     0           normal_left3_1[0][0]             \n",
      "                                                                 adjust_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_1 (Add)            (None, 4, 4, 44)     0           normal_left4_1[0][0]             \n",
      "                                                                 normal_right4_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_1 (Add)            (None, 4, 4, 44)     0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_1 (Concatenate)   (None, 4, 4, 264)    0           adjust_bn_1[0][0]                \n",
      "                                                                 normal_add_1_1[0][0]             \n",
      "                                                                 normal_add_2_1[0][0]             \n",
      "                                                                 normal_add_3_1[0][0]             \n",
      "                                                                 normal_add_4_1[0][0]             \n",
      "                                                                 normal_add_5_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 4, 264)    0           normal_concat_0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 4, 264)    0           normal_concat_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_2 (Conv2 (None, 4, 4, 44)     11616       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_2 (Conv2D)        (None, 4, 4, 44)     11616       activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_2 (BatchNormalization (None, 4, 4, 44)     176         adjust_conv_projection_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_2 (BatchNormalizati (None, 4, 4, 44)     176         normal_conv_1_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 44)     0           normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 4, 4, 44)     0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 4, 4, 44)     0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 4, 4, 44)     0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 4, 4, 44)     0           normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_2 (None, 4, 4, 44)     3036        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 4, 4, 44)     2332        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_2 (None, 4, 4, 44)     3036        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 4, 4, 44)     2332        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_2 (None, 4, 4, 44)     2332        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 4, 4, 44)     176         separable_conv_1_normal_left1_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_1_normal_right1_2[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 4, 4, 44)     176         separable_conv_1_normal_left2_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_1_normal_right2_2[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 4, 4, 44)     176         separable_conv_1_normal_left5_2[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_2 (None, 4, 4, 44)     3036        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 4, 4, 44)     2332        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_2 (None, 4, 4, 44)     3036        activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 4, 4, 44)     2332        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_2 (None, 4, 4, 44)     2332        activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 4, 4, 44)     176         separable_conv_2_normal_left1_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_2_normal_right1_2[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 4, 4, 44)     176         separable_conv_2_normal_left2_2[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_2_normal_right2_2[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_2 (AveragePooling2 (None, 4, 4, 44)     0           normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_2 (AveragePooling2 (None, 4, 4, 44)     0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_2 (AveragePooling (None, 4, 4, 44)     0           adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 4, 4, 44)     176         separable_conv_2_normal_left5_2[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_2 (Add)            (None, 4, 4, 44)     0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_2 (Add)            (None, 4, 4, 44)     0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_2 (Add)            (None, 4, 4, 44)     0           normal_left3_2[0][0]             \n",
      "                                                                 adjust_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_2 (Add)            (None, 4, 4, 44)     0           normal_left4_2[0][0]             \n",
      "                                                                 normal_right4_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_2 (Add)            (None, 4, 4, 44)     0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_2 (Concatenate)   (None, 4, 4, 264)    0           adjust_bn_2[0][0]                \n",
      "                                                                 normal_add_1_2[0][0]             \n",
      "                                                                 normal_add_2_2[0][0]             \n",
      "                                                                 normal_add_3_2[0][0]             \n",
      "                                                                 normal_add_4_2[0][0]             \n",
      "                                                                 normal_add_5_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 4, 4, 264)    0           normal_concat_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 4, 4, 264)    0           normal_concat_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_3 (Conv2 (None, 4, 4, 44)     11616       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_3 (Conv2D)        (None, 4, 4, 44)     11616       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_3 (BatchNormalization (None, 4, 4, 44)     176         adjust_conv_projection_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_3 (BatchNormalizati (None, 4, 4, 44)     176         normal_conv_1_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 4, 4, 44)     0           normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 4, 4, 44)     0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 4, 4, 44)     0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 4, 4, 44)     0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 4, 4, 44)     0           normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_3 (None, 4, 4, 44)     3036        activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 4, 4, 44)     2332        activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_3 (None, 4, 4, 44)     3036        activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 4, 4, 44)     2332        activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_3 (None, 4, 4, 44)     2332        activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 4, 4, 44)     176         separable_conv_1_normal_left1_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_1_normal_right1_3[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 4, 4, 44)     176         separable_conv_1_normal_left2_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_1_normal_right2_3[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 4, 4, 44)     176         separable_conv_1_normal_left5_3[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 4, 4, 44)     0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_3 (None, 4, 4, 44)     3036        activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 4, 4, 44)     2332        activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_3 (None, 4, 4, 44)     3036        activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 4, 4, 44)     2332        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_3 (None, 4, 4, 44)     2332        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 4, 4, 44)     176         separable_conv_2_normal_left1_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_2_normal_right1_3[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 4, 4, 44)     176         separable_conv_2_normal_left2_3[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 4, 4, 44)     176         separable_conv_2_normal_right2_3[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_3 (AveragePooling2 (None, 4, 4, 44)     0           normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_3 (AveragePooling2 (None, 4, 4, 44)     0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_3 (AveragePooling (None, 4, 4, 44)     0           adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 4, 4, 44)     176         separable_conv_2_normal_left5_3[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_3 (Add)            (None, 4, 4, 44)     0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_3 (Add)            (None, 4, 4, 44)     0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_3 (Add)            (None, 4, 4, 44)     0           normal_left3_3[0][0]             \n",
      "                                                                 adjust_bn_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_3 (Add)            (None, 4, 4, 44)     0           normal_left4_3[0][0]             \n",
      "                                                                 normal_right4_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_3 (Add)            (None, 4, 4, 44)     0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_3 (Concatenate)   (None, 4, 4, 264)    0           adjust_bn_3[0][0]                \n",
      "                                                                 normal_add_1_3[0][0]             \n",
      "                                                                 normal_add_2_3[0][0]             \n",
      "                                                                 normal_add_3_3[0][0]             \n",
      "                                                                 normal_add_4_3[0][0]             \n",
      "                                                                 normal_add_5_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 4, 4, 264)    0           normal_concat_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 4, 4, 264)    0           normal_concat_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_reduce_4 (Conv (None, 4, 4, 88)     23232       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_reduce_4 (None, 4, 4, 88)     23232       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_reduce_4 (BatchN (None, 4, 4, 88)     352         reduction_conv_1_reduce_4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_reduce_4 (BatchNormal (None, 4, 4, 88)     352         adjust_conv_projection_reduce_4[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 4, 4, 88)     0           reduction_bn_1_reduce_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 4, 4, 88)     0           adjust_bn_reduce_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 7, 7, 88)     0           activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 9, 9, 88)     0           activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 2, 2, 88)     9944        separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 2, 2, 88)     12056       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 2, 2, 88)     352         separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 2, 2, 88)     352         separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 2, 2, 88)     0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 2, 2, 88)     0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 2, 2, 88)     9944        activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 2, 2, 88)     12056       activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 4, 4, 88)     0           adjust_bn_reduce_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 2, 2, 88)     352         separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 2, 2, 88)     352         separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 9, 9, 88)     0           activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 4, 4, 88)     0           adjust_bn_reduce_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_reduce_4 (Add)  (None, 2, 2, 88)     0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 2, 2, 88)     12056       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 7, 7, 88)     0           activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 2, 2, 88)     0           reduction_add_1_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 2, 2, 88)     352         separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 2, 2, 88)     9944        separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 2, 2, 88)     8536        activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 2, 2, 88)     0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 2, 2, 88)     352         separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 2, 2, 88)     352         separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_reduce_4 (ZeroP (None, 5, 5, 88)     0           reduction_bn_1_reduce_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 2, 2, 88)     12056       activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 2, 2, 88)     0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 2, 2, 88)     0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_reduce_4 (MaxPo (None, 2, 2, 88)     0           reduction_pad_1_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 2, 2, 88)     352         separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 2, 2, 88)     9944        activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 2, 2, 88)     8536        activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_5 (Activation)    (None, 4, 4, 264)    0           normal_concat_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_reduce_4 (Add)  (None, 2, 2, 88)     0           reduction_left2_reduce_4[0][0]   \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_reduce_4 (Avera (None, 2, 2, 88)     0           reduction_pad_1_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 2, 2, 88)     352         separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_reduce_4 (Avera (None, 2, 2, 88)     0           reduction_add_1_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 2, 2, 88)     352         separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_reduce_4 (MaxP (None, 2, 2, 88)     0           reduction_pad_1_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 5, 5, 264)    0           adjust_relu_1_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_reduce_4 (Add)   (None, 2, 2, 88)     0           reduction_left3_reduce_4[0][0]   \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 2, 2, 88)     0           reduction_add_2_reduce_4[0][0]   \n",
      "                                                                 reduction_left4_reduce_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_reduce_4 (Add)   (None, 2, 2, 88)     0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_reduce_4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_3 (Cropping2D)       (None, 4, 4, 264)    0           zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_reduce_4 (Conc (None, 2, 2, 352)    0           reduction_add_2_reduce_4[0][0]   \n",
      "                                                                 reduction_add3_reduce_4[0][0]    \n",
      "                                                                 add_3[0][0]                      \n",
      "                                                                 reduction_add4_reduce_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_5 (AveragePoo (None, 2, 2, 264)    0           adjust_relu_1_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_5 (AveragePoo (None, 2, 2, 264)    0           cropping2d_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_5 (Conv2D)        (None, 2, 2, 44)     11616       adjust_avg_pool_1_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_5 (Conv2D)        (None, 2, 2, 44)     11616       adjust_avg_pool_2_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 2, 2, 352)    0           reduction_concat_reduce_4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 2, 2, 88)     0           adjust_conv_1_5[0][0]            \n",
      "                                                                 adjust_conv_2_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_5 (Conv2D)        (None, 2, 2, 88)     30976       activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_5 (BatchNormalization (None, 2, 2, 88)     352         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_5 (BatchNormalizati (None, 2, 2, 88)     352         normal_conv_1_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 2, 2, 88)     0           normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 2, 2, 88)     0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 2, 2, 88)     0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 2, 2, 88)     0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 2, 2, 88)     0           normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_5 (None, 2, 2, 88)     9944        activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 2, 2, 88)     8536        activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_5 (None, 2, 2, 88)     9944        activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 2, 2, 88)     8536        activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_5 (None, 2, 2, 88)     8536        activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 2, 88)     352         separable_conv_1_normal_left1_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_1_normal_right1_5[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 2, 88)     352         separable_conv_1_normal_left2_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_1_normal_right2_5[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 2, 88)     352         separable_conv_1_normal_left5_5[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 2, 2, 88)     0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 2, 2, 88)     0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 2, 2, 88)     0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 2, 2, 88)     0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 2, 2, 88)     0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_5 (None, 2, 2, 88)     9944        activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 2, 2, 88)     8536        activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_5 (None, 2, 2, 88)     9944        activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 2, 2, 88)     8536        activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_5 (None, 2, 2, 88)     8536        activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 2, 88)     352         separable_conv_2_normal_left1_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_2_normal_right1_5[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 2, 88)     352         separable_conv_2_normal_left2_5[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_2_normal_right2_5[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_5 (AveragePooling2 (None, 2, 2, 88)     0           normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_5 (AveragePooling2 (None, 2, 2, 88)     0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_5 (AveragePooling (None, 2, 2, 88)     0           adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 2, 88)     352         separable_conv_2_normal_left5_5[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_5 (Add)            (None, 2, 2, 88)     0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_5 (Add)            (None, 2, 2, 88)     0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_5 (Add)            (None, 2, 2, 88)     0           normal_left3_5[0][0]             \n",
      "                                                                 adjust_bn_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_5 (Add)            (None, 2, 2, 88)     0           normal_left4_5[0][0]             \n",
      "                                                                 normal_right4_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_5 (Add)            (None, 2, 2, 88)     0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_5 (Concatenate)   (None, 2, 2, 528)    0           adjust_bn_5[0][0]                \n",
      "                                                                 normal_add_1_5[0][0]             \n",
      "                                                                 normal_add_2_5[0][0]             \n",
      "                                                                 normal_add_3_5[0][0]             \n",
      "                                                                 normal_add_4_5[0][0]             \n",
      "                                                                 normal_add_5_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 2, 2, 352)    0           reduction_concat_reduce_4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 2, 2, 528)    0           normal_concat_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_6 (Conv2 (None, 2, 2, 88)     30976       activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_6 (Conv2D)        (None, 2, 2, 88)     46464       activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_6 (BatchNormalization (None, 2, 2, 88)     352         adjust_conv_projection_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_6 (BatchNormalizati (None, 2, 2, 88)     352         normal_conv_1_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 2, 2, 88)     0           normal_bn_1_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 2, 2, 88)     0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 2, 2, 88)     0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 2, 2, 88)     0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 2, 2, 88)     0           normal_bn_1_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_6 (None, 2, 2, 88)     9944        activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 2, 2, 88)     8536        activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_6 (None, 2, 2, 88)     9944        activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 2, 2, 88)     8536        activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_6 (None, 2, 2, 88)     8536        activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 2, 88)     352         separable_conv_1_normal_left1_6[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_1_normal_right1_6[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 2, 88)     352         separable_conv_1_normal_left2_6[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_1_normal_right2_6[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 2, 88)     352         separable_conv_1_normal_left5_6[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 2, 2, 88)     0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 2, 2, 88)     0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 2, 2, 88)     0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 2, 2, 88)     0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 2, 2, 88)     0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_6 (None, 2, 2, 88)     9944        activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 2, 2, 88)     8536        activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_6 (None, 2, 2, 88)     9944        activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 2, 2, 88)     8536        activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_6 (None, 2, 2, 88)     8536        activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 2, 88)     352         separable_conv_2_normal_left1_6[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_2_normal_right1_6[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 2, 88)     352         separable_conv_2_normal_left2_6[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_2_normal_right2_6[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_6 (AveragePooling2 (None, 2, 2, 88)     0           normal_bn_1_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_6 (AveragePooling2 (None, 2, 2, 88)     0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_6 (AveragePooling (None, 2, 2, 88)     0           adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 2, 88)     352         separable_conv_2_normal_left5_6[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_6 (Add)            (None, 2, 2, 88)     0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_6 (Add)            (None, 2, 2, 88)     0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_6 (Add)            (None, 2, 2, 88)     0           normal_left3_6[0][0]             \n",
      "                                                                 adjust_bn_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_6 (Add)            (None, 2, 2, 88)     0           normal_left4_6[0][0]             \n",
      "                                                                 normal_right4_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_6 (Add)            (None, 2, 2, 88)     0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_6 (Concatenate)   (None, 2, 2, 528)    0           adjust_bn_6[0][0]                \n",
      "                                                                 normal_add_1_6[0][0]             \n",
      "                                                                 normal_add_2_6[0][0]             \n",
      "                                                                 normal_add_3_6[0][0]             \n",
      "                                                                 normal_add_4_6[0][0]             \n",
      "                                                                 normal_add_5_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 2, 2, 528)    0           normal_concat_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 2, 2, 528)    0           normal_concat_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_7 (Conv2 (None, 2, 2, 88)     46464       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_7 (Conv2D)        (None, 2, 2, 88)     46464       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_7 (BatchNormalization (None, 2, 2, 88)     352         adjust_conv_projection_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_7 (BatchNormalizati (None, 2, 2, 88)     352         normal_conv_1_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 2, 2, 88)     0           normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 2, 2, 88)     0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 2, 2, 88)     0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 2, 2, 88)     0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 2, 2, 88)     0           normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_7 (None, 2, 2, 88)     9944        activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 2, 2, 88)     8536        activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_7 (None, 2, 2, 88)     9944        activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 2, 2, 88)     8536        activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_7 (None, 2, 2, 88)     8536        activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 2, 88)     352         separable_conv_1_normal_left1_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_1_normal_right1_7[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 2, 88)     352         separable_conv_1_normal_left2_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_1_normal_right2_7[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 2, 88)     352         separable_conv_1_normal_left5_7[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 2, 2, 88)     0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 2, 2, 88)     0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 2, 2, 88)     0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 2, 2, 88)     0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 2, 2, 88)     0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_7 (None, 2, 2, 88)     9944        activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 2, 2, 88)     8536        activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_7 (None, 2, 2, 88)     9944        activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 2, 2, 88)     8536        activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_7 (None, 2, 2, 88)     8536        activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 2, 88)     352         separable_conv_2_normal_left1_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_2_normal_right1_7[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 2, 88)     352         separable_conv_2_normal_left2_7[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_2_normal_right2_7[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_7 (AveragePooling2 (None, 2, 2, 88)     0           normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_7 (AveragePooling2 (None, 2, 2, 88)     0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_7 (AveragePooling (None, 2, 2, 88)     0           adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 2, 88)     352         separable_conv_2_normal_left5_7[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_7 (Add)            (None, 2, 2, 88)     0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_7 (Add)            (None, 2, 2, 88)     0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_7 (Add)            (None, 2, 2, 88)     0           normal_left3_7[0][0]             \n",
      "                                                                 adjust_bn_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_7 (Add)            (None, 2, 2, 88)     0           normal_left4_7[0][0]             \n",
      "                                                                 normal_right4_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_7 (Add)            (None, 2, 2, 88)     0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_7 (Concatenate)   (None, 2, 2, 528)    0           adjust_bn_7[0][0]                \n",
      "                                                                 normal_add_1_7[0][0]             \n",
      "                                                                 normal_add_2_7[0][0]             \n",
      "                                                                 normal_add_3_7[0][0]             \n",
      "                                                                 normal_add_4_7[0][0]             \n",
      "                                                                 normal_add_5_7[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 2, 2, 528)    0           normal_concat_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 2, 2, 528)    0           normal_concat_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_8 (Conv2 (None, 2, 2, 88)     46464       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_8 (Conv2D)        (None, 2, 2, 88)     46464       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_8 (BatchNormalization (None, 2, 2, 88)     352         adjust_conv_projection_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_8 (BatchNormalizati (None, 2, 2, 88)     352         normal_conv_1_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 2, 2, 88)     0           normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 2, 2, 88)     0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 2, 2, 88)     0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 2, 2, 88)     0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 2, 2, 88)     0           normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_8 (None, 2, 2, 88)     9944        activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 2, 2, 88)     8536        activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_8 (None, 2, 2, 88)     9944        activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 2, 2, 88)     8536        activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_8 (None, 2, 2, 88)     8536        activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 2, 88)     352         separable_conv_1_normal_left1_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_1_normal_right1_8[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 2, 88)     352         separable_conv_1_normal_left2_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_1_normal_right2_8[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 2, 2, 88)     352         separable_conv_1_normal_left5_8[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 2, 2, 88)     0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 2, 2, 88)     0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 2, 2, 88)     0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 2, 2, 88)     0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 2, 2, 88)     0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_8 (None, 2, 2, 88)     9944        activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 2, 2, 88)     8536        activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_8 (None, 2, 2, 88)     9944        activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 2, 2, 88)     8536        activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_8 (None, 2, 2, 88)     8536        activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 2, 88)     352         separable_conv_2_normal_left1_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_2_normal_right1_8[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 2, 88)     352         separable_conv_2_normal_left2_8[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 2, 2, 88)     352         separable_conv_2_normal_right2_8[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_8 (AveragePooling2 (None, 2, 2, 88)     0           normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_8 (AveragePooling2 (None, 2, 2, 88)     0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_8 (AveragePooling (None, 2, 2, 88)     0           adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 2, 2, 88)     352         separable_conv_2_normal_left5_8[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_8 (Add)            (None, 2, 2, 88)     0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_8 (Add)            (None, 2, 2, 88)     0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_8 (Add)            (None, 2, 2, 88)     0           normal_left3_8[0][0]             \n",
      "                                                                 adjust_bn_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_8 (Add)            (None, 2, 2, 88)     0           normal_left4_8[0][0]             \n",
      "                                                                 normal_right4_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_8 (Add)            (None, 2, 2, 88)     0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_8 (Concatenate)   (None, 2, 2, 528)    0           adjust_bn_8[0][0]                \n",
      "                                                                 normal_add_1_8[0][0]             \n",
      "                                                                 normal_add_2_8[0][0]             \n",
      "                                                                 normal_add_3_8[0][0]             \n",
      "                                                                 normal_add_4_8[0][0]             \n",
      "                                                                 normal_add_5_8[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 2, 2, 528)    0           normal_concat_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 2, 2, 528)    0           normal_concat_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_conv_1_reduce_8 (Conv (None, 2, 2, 176)    92928       activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_reduce_8 (None, 2, 2, 176)    92928       activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reduction_bn_1_reduce_8 (BatchN (None, 2, 2, 176)    704         reduction_conv_1_reduce_8[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_reduce_8 (BatchNormal (None, 2, 2, 176)    704         adjust_conv_projection_reduce_8[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 2, 2, 176)    0           reduction_bn_1_reduce_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 2, 2, 176)    0           adjust_bn_reduce_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 5, 5, 176)    0           activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 7, 7, 176)    0           activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 1, 1, 176)    35376       separable_conv_1_pad_reduction_le\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 1, 1, 176)    39600       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 1, 1, 176)    704         separable_conv_1_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 1, 1, 176)    704         separable_conv_1_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 1, 1, 176)    35376       activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 1, 1, 176)    39600       activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 2, 2, 176)    0           adjust_bn_reduce_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 1, 1, 176)    704         separable_conv_2_reduction_left1_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 1, 1, 176)    704         separable_conv_2_reduction_right1\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 7, 7, 176)    0           activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 2, 2, 176)    0           adjust_bn_reduce_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_1_reduce_8 (Add)  (None, 1, 1, 176)    0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 1, 1, 176)    39600       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_pad_reduction_ (None, 5, 5, 176)    0           activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 1, 1, 176)    0           reduction_add_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 1, 1, 176)    704         separable_conv_1_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_righ (None, 1, 1, 176)    35376       separable_conv_1_pad_reduction_ri\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_reduction_left (None, 1, 1, 176)    32560       activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_r (None, 1, 1, 176)    704         separable_conv_1_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_reduction_l (None, 1, 1, 176)    704         separable_conv_1_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_pad_1_reduce_8 (ZeroP (None, 3, 3, 176)    0           reduction_bn_1_reduce_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 1, 1, 176)    39600       activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_reduction_lef\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left2_reduce_8 (MaxPo (None, 1, 1, 176)    0           reduction_pad_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 1, 1, 176)    704         separable_conv_2_reduction_right2\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_righ (None, 1, 1, 176)    35376       activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_reduction_left (None, 1, 1, 176)    32560       activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_relu_1_9 (Activation)    (None, 2, 2, 528)    0           normal_concat_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add_2_reduce_8 (Add)  (None, 1, 1, 176)    0           reduction_left2_reduce_8[0][0]   \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left3_reduce_8 (Avera (None, 1, 1, 176)    0           reduction_pad_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_r (None, 1, 1, 176)    704         separable_conv_2_reduction_right3\n",
      "__________________________________________________________________________________________________\n",
      "reduction_left4_reduce_8 (Avera (None, 1, 1, 176)    0           reduction_add_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_reduction_l (None, 1, 1, 176)    704         separable_conv_2_reduction_left4_\n",
      "__________________________________________________________________________________________________\n",
      "reduction_right5_reduce_8 (MaxP (None, 1, 1, 176)    0           reduction_pad_1_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 3, 3, 528)    0           adjust_relu_1_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add3_reduce_8 (Add)   (None, 1, 1, 176)    0           reduction_left3_reduce_8[0][0]   \n",
      "                                                                 separable_conv_2_bn_reduction_rig\n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 1, 1, 176)    0           reduction_add_2_reduce_8[0][0]   \n",
      "                                                                 reduction_left4_reduce_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reduction_add4_reduce_8 (Add)   (None, 1, 1, 176)    0           separable_conv_2_bn_reduction_lef\n",
      "                                                                 reduction_right5_reduce_8[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_4 (Cropping2D)       (None, 2, 2, 528)    0           zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reduction_concat_reduce_8 (Conc (None, 1, 1, 704)    0           reduction_add_2_reduce_8[0][0]   \n",
      "                                                                 reduction_add3_reduce_8[0][0]    \n",
      "                                                                 add_4[0][0]                      \n",
      "                                                                 reduction_add4_reduce_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_1_9 (AveragePoo (None, 1, 1, 528)    0           adjust_relu_1_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_avg_pool_2_9 (AveragePoo (None, 1, 1, 528)    0           cropping2d_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_1_9 (Conv2D)        (None, 1, 1, 88)     46464       adjust_avg_pool_1_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_2_9 (Conv2D)        (None, 1, 1, 88)     46464       adjust_avg_pool_2_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 1, 1, 704)    0           reduction_concat_reduce_8[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1, 1, 176)    0           adjust_conv_1_9[0][0]            \n",
      "                                                                 adjust_conv_2_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_9 (Conv2D)        (None, 1, 1, 176)    123904      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_9 (BatchNormalization (None, 1, 1, 176)    704         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_9 (BatchNormalizati (None, 1, 1, 176)    704         normal_conv_1_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 1, 1, 176)    0           normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 1, 1, 176)    0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 1, 1, 176)    0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 1, 1, 176)    0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 1, 1, 176)    0           normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_9 (None, 1, 1, 176)    35376       activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 1, 1, 176)    32560       activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_9 (None, 1, 1, 176)    35376       activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 1, 1, 176)    32560       activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_9 (None, 1, 1, 176)    32560       activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 1, 1, 176)    704         separable_conv_1_normal_left1_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_1_normal_right1_9[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 1, 1, 176)    704         separable_conv_1_normal_left2_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_1_normal_right2_9[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 1, 1, 176)    704         separable_conv_1_normal_left5_9[0\n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_9 (None, 1, 1, 176)    35376       activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 1, 1, 176)    32560       activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_9 (None, 1, 1, 176)    35376       activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 1, 1, 176)    32560       activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_9 (None, 1, 1, 176)    32560       activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 1, 1, 176)    704         separable_conv_2_normal_left1_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_2_normal_right1_9[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 1, 1, 176)    704         separable_conv_2_normal_left2_9[0\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_2_normal_right2_9[\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_9 (AveragePooling2 (None, 1, 1, 176)    0           normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_9 (AveragePooling2 (None, 1, 1, 176)    0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_9 (AveragePooling (None, 1, 1, 176)    0           adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 1, 1, 176)    704         separable_conv_2_normal_left5_9[0\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_9 (Add)            (None, 1, 1, 176)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_9 (Add)            (None, 1, 1, 176)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_9 (Add)            (None, 1, 1, 176)    0           normal_left3_9[0][0]             \n",
      "                                                                 adjust_bn_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_9 (Add)            (None, 1, 1, 176)    0           normal_left4_9[0][0]             \n",
      "                                                                 normal_right4_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_9 (Add)            (None, 1, 1, 176)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_9 (Concatenate)   (None, 1, 1, 1056)   0           adjust_bn_9[0][0]                \n",
      "                                                                 normal_add_1_9[0][0]             \n",
      "                                                                 normal_add_2_9[0][0]             \n",
      "                                                                 normal_add_3_9[0][0]             \n",
      "                                                                 normal_add_4_9[0][0]             \n",
      "                                                                 normal_add_5_9[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 1, 1, 704)    0           reduction_concat_reduce_8[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 1, 1, 1056)   0           normal_concat_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_10 (Conv (None, 1, 1, 176)    123904      activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_10 (Conv2D)       (None, 1, 1, 176)    185856      activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_10 (BatchNormalizatio (None, 1, 1, 176)    704         adjust_conv_projection_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_10 (BatchNormalizat (None, 1, 1, 176)    704         normal_conv_1_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 1, 1, 176)    0           normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 1, 1, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 1, 1, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 1, 1, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 1, 1, 176)    0           normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 1, 1, 176)    35376       activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 1, 1, 176)    32560       activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 1, 1, 176)    35376       activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 1, 1, 176)    32560       activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 1, 1, 176)    32560       activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 1, 1, 176)    704         separable_conv_1_normal_left1_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_1_normal_right1_10\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 1, 1, 176)    704         separable_conv_1_normal_left2_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_1_normal_right2_10\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 1, 1, 176)    704         separable_conv_1_normal_left5_10[\n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 1, 1, 176)    35376       activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 1, 1, 176)    32560       activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 1, 1, 176)    35376       activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 1, 1, 176)    32560       activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 1, 1, 176)    32560       activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 1, 1, 176)    704         separable_conv_2_normal_left1_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_2_normal_right1_10\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 1, 1, 176)    704         separable_conv_2_normal_left2_10[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_2_normal_right2_10\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_10 (AveragePooling (None, 1, 1, 176)    0           normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_10 (AveragePooling (None, 1, 1, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_10 (AveragePoolin (None, 1, 1, 176)    0           adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 1, 1, 176)    704         separable_conv_2_normal_left5_10[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_10 (Add)           (None, 1, 1, 176)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_10 (Add)           (None, 1, 1, 176)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_10 (Add)           (None, 1, 1, 176)    0           normal_left3_10[0][0]            \n",
      "                                                                 adjust_bn_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_10 (Add)           (None, 1, 1, 176)    0           normal_left4_10[0][0]            \n",
      "                                                                 normal_right4_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_10 (Add)           (None, 1, 1, 176)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_10 (Concatenate)  (None, 1, 1, 1056)   0           adjust_bn_10[0][0]               \n",
      "                                                                 normal_add_1_10[0][0]            \n",
      "                                                                 normal_add_2_10[0][0]            \n",
      "                                                                 normal_add_3_10[0][0]            \n",
      "                                                                 normal_add_4_10[0][0]            \n",
      "                                                                 normal_add_5_10[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 1, 1, 1056)   0           normal_concat_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 1, 1, 1056)   0           normal_concat_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_11 (Conv (None, 1, 1, 176)    185856      activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_11 (Conv2D)       (None, 1, 1, 176)    185856      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_11 (BatchNormalizatio (None, 1, 1, 176)    704         adjust_conv_projection_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_11 (BatchNormalizat (None, 1, 1, 176)    704         normal_conv_1_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 1, 1, 176)    0           normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 1, 1, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 1, 1, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 1, 1, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 1, 1, 176)    0           normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 1, 1, 176)    35376       activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 1, 1, 176)    32560       activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 1, 1, 176)    35376       activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 1, 1, 176)    32560       activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 1, 1, 176)    32560       activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 1, 1, 176)    704         separable_conv_1_normal_left1_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_1_normal_right1_11\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 1, 1, 176)    704         separable_conv_1_normal_left2_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_1_normal_right2_11\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 1, 1, 176)    704         separable_conv_1_normal_left5_11[\n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 1, 1, 176)    35376       activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 1, 1, 176)    32560       activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 1, 1, 176)    35376       activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 1, 1, 176)    32560       activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 1, 1, 176)    32560       activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 1, 1, 176)    704         separable_conv_2_normal_left1_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_2_normal_right1_11\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 1, 1, 176)    704         separable_conv_2_normal_left2_11[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_2_normal_right2_11\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_11 (AveragePooling (None, 1, 1, 176)    0           normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_11 (AveragePooling (None, 1, 1, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_11 (AveragePoolin (None, 1, 1, 176)    0           adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 1, 1, 176)    704         separable_conv_2_normal_left5_11[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_11 (Add)           (None, 1, 1, 176)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_11 (Add)           (None, 1, 1, 176)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_11 (Add)           (None, 1, 1, 176)    0           normal_left3_11[0][0]            \n",
      "                                                                 adjust_bn_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_11 (Add)           (None, 1, 1, 176)    0           normal_left4_11[0][0]            \n",
      "                                                                 normal_right4_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_11 (Add)           (None, 1, 1, 176)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_11 (Concatenate)  (None, 1, 1, 1056)   0           adjust_bn_11[0][0]               \n",
      "                                                                 normal_add_1_11[0][0]            \n",
      "                                                                 normal_add_2_11[0][0]            \n",
      "                                                                 normal_add_3_11[0][0]            \n",
      "                                                                 normal_add_4_11[0][0]            \n",
      "                                                                 normal_add_5_11[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 1, 1, 1056)   0           normal_concat_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 1, 1, 1056)   0           normal_concat_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adjust_conv_projection_12 (Conv (None, 1, 1, 176)    185856      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_conv_1_12 (Conv2D)       (None, 1, 1, 176)    185856      activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "adjust_bn_12 (BatchNormalizatio (None, 1, 1, 176)    704         adjust_conv_projection_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normal_bn_1_12 (BatchNormalizat (None, 1, 1, 176)    704         normal_conv_1_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 1, 1, 176)    0           normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 1, 1, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 1, 1, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 1, 1, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 1, 1, 176)    0           normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left1_1 (None, 1, 1, 176)    35376       activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right1_ (None, 1, 1, 176)    32560       activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left2_1 (None, 1, 1, 176)    35376       activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_right2_ (None, 1, 1, 176)    32560       activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_normal_left5_1 (None, 1, 1, 176)    32560       activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 1, 1, 176)    704         separable_conv_1_normal_left1_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_1_normal_right1_12\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 1, 1, 176)    704         separable_conv_1_normal_left2_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_1_normal_right2_12\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_1_bn_normal_left (None, 1, 1, 176)    704         separable_conv_1_normal_left5_12[\n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_left1_\n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_left2_\n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 1, 1, 176)    0           separable_conv_1_bn_normal_left5_\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left1_1 (None, 1, 1, 176)    35376       activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right1_ (None, 1, 1, 176)    32560       activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left2_1 (None, 1, 1, 176)    35376       activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_right2_ (None, 1, 1, 176)    32560       activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_normal_left5_1 (None, 1, 1, 176)    32560       activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 1, 1, 176)    704         separable_conv_2_normal_left1_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_2_normal_right1_12\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 1, 1, 176)    704         separable_conv_2_normal_left2_12[\n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_righ (None, 1, 1, 176)    704         separable_conv_2_normal_right2_12\n",
      "__________________________________________________________________________________________________\n",
      "normal_left3_12 (AveragePooling (None, 1, 1, 176)    0           normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_left4_12 (AveragePooling (None, 1, 1, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_right4_12 (AveragePoolin (None, 1, 1, 176)    0           adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv_2_bn_normal_left (None, 1, 1, 176)    704         separable_conv_2_normal_left5_12[\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_1_12 (Add)           (None, 1, 1, 176)    0           separable_conv_2_bn_normal_left1_\n",
      "                                                                 separable_conv_2_bn_normal_right1\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_2_12 (Add)           (None, 1, 1, 176)    0           separable_conv_2_bn_normal_left2_\n",
      "                                                                 separable_conv_2_bn_normal_right2\n",
      "__________________________________________________________________________________________________\n",
      "normal_add_3_12 (Add)           (None, 1, 1, 176)    0           normal_left3_12[0][0]            \n",
      "                                                                 adjust_bn_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_4_12 (Add)           (None, 1, 1, 176)    0           normal_left4_12[0][0]            \n",
      "                                                                 normal_right4_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "normal_add_5_12 (Add)           (None, 1, 1, 176)    0           separable_conv_2_bn_normal_left5_\n",
      "                                                                 normal_bn_1_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "normal_concat_12 (Concatenate)  (None, 1, 1, 1056)   0           adjust_bn_12[0][0]               \n",
      "                                                                 normal_add_1_12[0][0]            \n",
      "                                                                 normal_add_2_12[0][0]            \n",
      "                                                                 normal_add_3_12[0][0]            \n",
      "                                                                 normal_add_4_12[0][0]            \n",
      "                                                                 normal_add_5_12[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 1, 1, 1056)   0           normal_concat_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 1056)         0           activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 100)          105700      global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 4,374,840\n",
      "Trainable params: 4,338,102\n",
      "Non-trainable params: 36,738\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import nasnet\n",
    "\n",
    "def create_nasnet():  \n",
    "    gmodel = nasnet.NASNetMobile(input_shape=(32,32,1), include_top=True, weights=None, input_tensor=None, pooling=None, classes=100)\n",
    "    return gmodel\n",
    "\n",
    "nasnet_model = create_nasnet()  \n",
    "nasnet_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc', 'mse'])\n",
    "#nasnet_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nasnet = nasnet_model.fit(x=X_train_vector, y=y_train_vector, batch_size=32, epochs=10, verbose=1, validation_data=(X_test_vector, y_test), shuffle=True) "
=======
    "y_test"
>>>>>>> 0132ea18061251a4d0db23b8c6e3f94a7a259d06
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
